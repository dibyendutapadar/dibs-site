[{"content":"Its very tempting to =jump to GenAI= now for all kind of problems, but before GenAI was there, there were these cute simple algorithms which did the work with quite nice bit of accuracy. The downside was that they needed data to be trained on first, and with GenAI we work with pre trained model (foundation models). So if you have data, and the use case is to classify, and you dont want to shell out huge computation and monetary resources, then ML models such as decision tree is a good bet.\nWhat is the The core idea of a decision tree? A decision tree tries to repeatedly answer:\n“Which single feature + threshold splits the data into the purest groups?”\nLet\u0026rsquo;s figure out with an Example:\nFor all further explanation we will be referring to a hypothetical dataset of user engagement with 3 Simple Features and the Target is to determine Churn.\nFeature Meaning DaysSinceLastUsed How many days since the customer last logged into the product AvgWeeklyMins_Lifetime Average weekly active minutes over the whole customer lifetime AvgWeeklyMins_LastMonth Average weekly active minutes over the last 4 weeks Churned Yes/No — target label What does a decision tree want to do in this churn scenario? It wants to repeatedly answer:\n“Which one simple business rule splits customers into the clearest churn/non-churn groups?”\nExamples of such rules:\n“If DaysSinceLastUsed \u0026gt; 30 days → high churn risk”\n“If AvgWeeklyMins_LastMonth \u0026lt; 15 → high churn risk”\n“If engagement dropped (last month \u0026lt; lifetime avg) → potential churn”\nA tree is just a system that keeps discovering these rules automatically.\nThe Tree’s First Task: Find the most useful split A decision tree looks at every feature and asks:\n“If I split on this feature, do I separate churners and non-churners cleanly (purely?”\nLet\u0026rsquo;s start with one feature: DaysSinceLastUsed\nTry splitting by DaysSinceLastUsed Possible business rules it considers:\n“If last used \u0026gt; 10 days”\n“If last used \u0026gt; 20 days”\n“If last used \u0026gt; 30 days”\n…\nFor each possible threshold, the model checks:\nDo customers on each side behave differently in churn outcomes?\nIf one side becomes “mostly churned” and the other “mostly not,” the split is good. So If we look at the churn/total for each bucket\nCondition YES Bucket (Churn/Total) NO Bucket (Churn/Total) \u0026gt;10 days 25/60 5/40 \u0026gt;20 days 22/40 8/60 \u0026gt;30 days 18/25 12/75 We want:\nWhich Split creates the cleanest separation between churners and non-churners\nEnters the concept of ==Gini== and ==Entropy==.\n1️⃣ What is Gini?- “How often would I be wrong?\u0026quot; Gini asks\nIf I randomly pick two customers from this group, how likely are they to belong to different classes?\nHigh Gini = very mixed group -\u0026gt; bad split Low Gini = pure group -\u0026gt; great split When a bucket is 50/50, Gini is at worst, When a bucket is all churn or all non-churn, Gini = 0, known as ==perfect purity==\n2️⃣ What is Entropy — “How much disorder is in this bucket?”** Think of entropy as:\nHow unpredictable is this group?\nHigh entropy → the label is random, uncertain\nLow entropy → you can predict churn with high confidence\nEntropy is highest when:\n50% churn / 50% not churn\n→ \u0026ldquo;maximum uncertainty\u0026rdquo; Entropy goes to zero when:\nEveryone churned\nor everyone stayed\n→ \u0026ldquo;no uncertainty\u0026rdquo;\n3️⃣ What is Information Gain — “How much uncertainty did the split remove?” Information Gain flips the thinking:\nInstead of measuring impurity, it measures improvement.\nInformation Gain = (Parent impurity) – (Weighted impurity of children)\nMeaning:\nA great split dramatically reduces uncertainty\nA bad split barely improves (or worsens) uncertainty\nInformation gain is calculated by the ==gain== in ==Gini== (purity) or ==Entropy== (Uncertainity)\nDecision tree calcultes the gain or loss at each split and pick the\nSplit at Lowest Gini (if Gini is used) or Lowest Entropy (if Entropy is used) Well this raises more question than it answers:\nHow are the initial split determined, how is the next split determined? How to know where to stop? How does this propagate for other features? We will try to explore them in part 2.\nLets get into the maths a bit You can skip this section. and go to Part 2\nStep 1 : Parent Impurity Across all 100 customers\nTotal Churned = 20 Total Not Churned = 80 so $p_c$ (Churn) = 20/100 =0.2 $p_n$ (Not Churn) = 80/100 =0.8 p(x) ~ probability of x happening\nParent Gini $$ G = 1 - (p_c^2 + p_n^2)$$ $$ G = 1- (0.2^2 + 0.8^2) = 1- 0.68 = 0.32 $$\nParent Gini = 0.32\nParent Entropy $$ H = -p_clog_2(p_c) - p_nlog_2(p_n)$$\n[!info] Why multiplying probability with log (probability) measure uncertainity? Claude Shanon intorduced information theory in 1948 $$ H= -0.2 \\times log_2(0.2) - 0.8 \\times log_2(0.8) $$ $$ H = 0.7219 $$\n","permalink":"http://localhost:52022/tech-for-pm/how-decision-tree-works/","summary":"How Decision tree works, the basics - split, gini, entropy, information gain","title":"How Decision Tree Works"},{"content":"For a while now, my LinkedIn feed has been a canvas for my thoughts and insights, a place where I’ve honed my perspectives and showcased my experiments.\nTo make things a little structured I though of launching my own portfolio and blog website to house these writings, expand on new topics by categories and connect more deeply with my audience. I built this website with a very effective Obsidian — Hugo Papermod - Netlify combo, and hosted via Cloudfare.\nNow, writing a lot is a lot of work, and that too constant writing. I initially turned to tools like ChatGPT to help generate initial drafts, hoping to speed up the process. While powerful, I quickly ran into a few persistent frustrations.\nThe back-and-forth required to get a decent draft was excessive Context was often forgotten in longer conversations, and most importantly, It consistently failed to capture my distinctive writing style — the tone, the rhythm, the specific way I phrase things.\nIt felt like I was spending more time editing to make it sound like me than I would have writing from scratch.\nThat’s when I decided to take matters into my own hands. I needed a solution that understood my style, remembered our interactions, and could quickly produce initial drafts that genuinely sounded like me. I was exploring agentic frameworks and thought, why not build an agent that solves my oen pain point first.\nIntroducing My Custom Article Writing Agent ADK This project, an article writing agent built using the Google ADK (Agent Development Kit) framework, is my solution to those content creation woes. It’s designed to automate the initial drafting process, allowing me to focus on refining, adding depth, and bringing my unique perspective to the forefront, rather than wrestling with generic AI output.\nHow It Works: A Collaborative Team of Agents Here’s a look at the workflow:\nGeneral Web Search Agent: Every great article starts with solid research. This agent kicks things off by performing in-depth web searches on a given topic and synthesizing key findings. It uses the google_search tool to gather the latest information, ensuring the content is well-informed and up-to-date. Aggregator Agent: Once the research is complete, the Aggregator Agent steps in. Its job is to take all the raw findings from the General Web Search Agent and synthesize them into a coherent, well-structured research brief. This brief then serves as the foundation for the drafting process. The Drafting Loop (The Heart of the System): This is where the magic of style matching truly happens, in an iterative cycle designed for continuous improvement: Writer Agent: Using the aggregated research brief, this agent creates the initial draft of the article. It focuses on clarity, structure, and engaging content. Style Review Agent: This is the secret sauce for personalization. Before it even looks at the draft, this agent calls a special tool (get_writing_style) that fetches my personal writing samples from a writing_style.md file. It then meticulously analyzes my tone, language, and overall \u0026ldquo;throw\u0026rdquo; to understand my unique voice. With that understanding, it suggests specific rewrites to the draft to align it perfectly with my established style. I’ve leveraged a more powerful model like gemini-2.5-pro here to ensure nuanced style analysis. Revision Agent: Acting as the lead editor, this agent takes input from both the initial writer and the style reviewer. It ensures the article is complete, incorporates all the user’s original pointers, and crucially, makes sure it matches my style. It’s designed to refine and summarize changes, pushing the article back into the loop for further iterations if needed (currently set for up to 2 cycles) until it’s polished. Formatter Agent: Finally, once the article has been through the drafting and revision loops and is fully approved, the Formatter Agent takes over. It transforms the content into clean, readable markdown, complete with appropriate headings, lists, bolding, italics, and blockquotes, ready for immediate publication. Here’s how you can get started with the Google ADK framework: Copy the repo from here : Github Install Google ADK: It’s as simple as pip install google-adk. Create an Agent: Use adk create \u0026lt;folder_name\u0026gt; to set up your project structure. Update Your **.env** File: Securely include your API key for authentication. Provide Your Writing Style Sample: This is critical for style matching. Create a writing_style.md file and fill it with samples of your writing. The more content you provide, the better the Style Review Agent can learn and replicate your unique voice. Refine Prompts: Tweak the prompts in the agent.py file to perfectly align with your specific needs and preferences. Run the Agent: Start your agent locally with adk run web and prompt it to bring your content ideas to well framed drafts in your own style. ","permalink":"http://localhost:52022/projects/how-i-built-a-writing-agent-using-google-adk-to-match-my-writing-style/","summary":"Mult Agent Orchestrated Blog writer using Google ADK, that learns and matches users writing style","title":"How I Built a Writing Agent using Google ADK to Match My Writing Style"},{"content":"Product Managers (PMs) used to treat AI like a distant, complex cousin – interesting if you were building the AI itself, but mostly irrelevant to the daily grind. Now, nearly every PM is wrestling with GenAI tools, and a surprising number are even trying to build their own. This isn\u0026rsquo;t just a trend; it\u0026rsquo;s a fundamental shift that\u0026rsquo;s re-engineering what it means to be a PM, all thanks to AI\u0026rsquo;s knack for demolishing the tedious, repetitive tasks that sucked the life out of our days.\nWe’ve always talked a good game about the core of PM:\nnavigating the organizational labyrinth, guiding the product ship to deliver value.\nBut the reality? Most of our time was eaten alive by a pile of manual, recurring, and frankly, soul-crushing tasks. These were the \u0026ldquo;unloved tasks,\u0026rdquo; the necessary evils that diluted our focus from strategy, innovation, and actually understanding our users.\nTake the Product Requirements Document (PRD). You’ve got the vision locked in your head, the user needs crystal clear. But translating that into a comprehensive, meticulously detailed PRD – one that anticipates every possible stakeholder question and engineering gotcha – was a monumental, laborious effort. Hours spent writing, formatting, agonizing. Then came generating user stories from that behemoth. Critical for development, sure, but an iterative process that could drag on for days. Data synthesis – not deep analysis, mind you, but the sheer grunt work of pulling disparate numbers into digestible formats for the execs – was another bottleneck. You needed the data to look \u0026ldquo;data-driven,\u0026rdquo; a staple in practically every PM job description. But getting that data often felt like pulling teeth. And who can forget the weekly presentation ritual? Compiling updates from a dozen sources, wrestling with slide formatting, ensuring each pixel was just right. Then, the ad-hoc presentations for the CEO, the CTO, the sales team – each with slightly different needs, demanding multiple revisions, rework, and a delicate dance to satisfy conflicting demands. This wasn\u0026rsquo;t the stuff of strategic genius; it was the stuff of burnout. This is where GenAI, especially Large Language Models (LLMs), has stepped in like a superhero. These tools are the ultimate task automators, directly targeting that \u0026ldquo;mindless, laborious grind\u0026rdquo; that defined so much of the PM workflow.\nIt’s a pattern we’ve seen before. History is littered with technologies that succeeded by automating the unloved tasks, freeing humans for what they actually valued:\nThe Industrial Revolution: Machines took over the back-breaking manual labor. We didn’t miss the grind; we loved managing the machines. Computers: Early computing and later spreadsheet software automated complex calculations. The appeal wasn\u0026rsquo;t in the tedious arithmetic but in the insights derived from the numbers. We loved the answers, not the calculation. The Washing Machine: This domestic marvel automated a chore that consumed hours. People embraced the convenience, choosing to spend their time on more engaging pursuits. We loved clean clothes, not the scrub. Now, LLMs are doing the same for Product Managers. They\u0026rsquo;re automating the laborious aspects of documentation, data aggregation, and presentation creation. This means PMs can reclaim their time and mental energy for the tasks that truly define their strategic value: crafting product strategy, making critical decisions, and deeply understanding and solving user problems.\nThe \u0026ldquo;Junior PM\u0026rdquo; Effect: Enter AI as Your New APM The impact of this automation is so profound that LLMs are effectively acting as a \u0026ldquo;junior PM\u0026rdquo; or an Associate Product Manager (APM). This has massive implications for the traditional APM role. Instead of spending their early careers drowning in document creation, data compilation, and basic task management, APMs can now leverage AI to handle these foundational elements.\nThis frees them to focus on what truly matters for their growth: genuine learning, in-depth user research, and a deep dive into problem areas. The emphasis shifts from being a document manager to becoming a true problem solver and strategic thinker from day one. Companies are already adapting, experimenting with roles that blend traditional PM skills with AI-assisted execution.\nThe future of product management lies not in a human versus AI battle, but in a powerful synergy. The most successful PMs will be those who can effectively blend AI-driven insights with their own deeply honed human judgment. The focus is shifting from the mechanics of managing backlogs to the art of managing intelligence. This requires critical discernment: knowing when AI genuinely adds transformative value and when it risks overcomplicating processes or providing superficial answers. The revolution is here, and it demands that product managers become masters of this new human-AI collaboration.\n","permalink":"http://localhost:52022/art-of-pm/how-ai-re-engineered-product-management/","summary":"\u003cp\u003eProduct Managers (PMs) used to treat AI like a distant, complex cousin – interesting if you were building the AI itself, but mostly irrelevant to the daily grind.  Now, nearly every PM is wrestling with GenAI tools, and a surprising number are even trying to build their own. This isn\u0026rsquo;t just a trend; it\u0026rsquo;s a fundamental shift that\u0026rsquo;s re-engineering what it means to be a PM, all thanks to AI\u0026rsquo;s knack for demolishing the tedious, repetitive tasks that sucked the life out of our days.\u003c/p\u003e","title":"How AI Re-Engineered Product Management"},{"content":"Ever wonder how your email app just knows what spam is? Or how Netflix uncannily suggests the exact B-movie you were secretly in the mood for? The answer is Machine Learning (ML).\nBefore ML, software was a bit of a bureaucrat. You had to write explicit, rigid if-this-then-that rules for every single possibility. This works fine until the real world, in all its messy glory, shows up. ML flips the script. Instead of feeding a machine rules, you feed it examples. The machine\u0026rsquo;s job is to look at thousands or millions of examples and figure out the patterns on its own.\nIn this post, we\u0026rsquo;ll skip the scary math and build your intuition for the four most common jobs we give to traditional ML:\nClassification: Is this A or B? Regression: How much of this will there be? Clustering: What are the natural groups in this data? Recommendations: Since you liked that, you\u0026rsquo;ll probably like this. Let\u0026rsquo;s dive in.\nClassification: The Art of Sorting Your Data Think of a classification model as a sorting hat for your data. Its one job is to look at a new piece of data and assign it to a predefined category or \u0026ldquo;class.\u0026rdquo; This is a type of supervised learning, which is a fancy way of saying we have to give the machine an answer key to learn from first.\nExample: Will This User Pay Up?\nA classic business problem: you have a ton of users on a free plan. Which ones are likely to convert to a paid subscription? This is a perfect sorting problem. The only two buckets are \u0026ldquo;Yes, they\u0026rsquo;ll convert\u0026rdquo; and \u0026ldquo;No, they won\u0026rsquo;t.\u0026rdquo;\nTo teach the machine, we\u0026rsquo;d give it a pile of historical data that looks something like this:\nUser ID Time Spent (hours) Features Used (count) Support Tickets (count) Last Login (days ago) Converted (Target) 101 5.2 3 1 5 Yes 102 1.1 1 0 20 No 103 8.5 5 2 2 Yes 104 2.0 2 0 15 No 105 6.1 4 1 3 Yes How does the algorithm work?\nAny algorithm is a three-step dance:\nPrep the Data: You gather all your historical user data, including that all-important Converted column (the \u0026ldquo;answer key\u0026rdquo;). You clean it up and select the features—the input signals—that you think might predict the outcome. This is more art than science. Train the Model: You feed this labeled data to a classification algorithm (its name doesn\u0026rsquo;t matter for now). The algorithm\u0026rsquo;s goal is to find the patterns. It might learn that users who spend more time on the app, use more features, and interact less with support are more likely to convert. It\u0026rsquo;s essentially drawing a line in the sand that best separates the \u0026ldquo;Yes\u0026rdquo; crowd from the \u0026ldquo;No\u0026rdquo; crowd. Make Predictions: Once the model is trained, it\u0026rsquo;s ready for the real world. You show it a new free user who has spent 4.5 hours, used 3 features, and last logged in 7 days ago. The model looks at this new data, compares it to the patterns it learned, and spits out a prediction: \u0026ldquo;Yes\u0026rdquo; or \u0026ldquo;No,\u0026rdquo; often with a confidence score. Regression: Predicting a Number Okay, sorting is cool. But what if you don\u0026rsquo;t need to put something in a bucket? What if you need to predict a specific, continuous number? That\u0026rsquo;s where regression comes in. It\u0026rsquo;s also supervised learning, but instead of predicting a label (Yes/No), it predicts a value (like $55,000).\nExample: How Much Will We Spend?\nImagine you\u0026rsquo;re trying to forecast next year\u0026rsquo;s operating expenses. You know that hiring more people costs more money, but by how much exactly? You want a model that takes your hiring plan and predicts the total cost.\nYour training data might look like this:\nMonth Department Level Planned Headcount Previous Quarter Expenses Operating Expenses (Target) Jan Engineering Senior 15 50,000 55,000 Jan Sales Junior 25 30,000 33,000 Feb Engineering Junior 10 45,000 48,000 Feb Marketing Manager 5 20,000 22,000 Mar Sales Senior 20 35,000 38,000 So, what features do we need?\nHeadcount Info: The number of hires, by department and seniority. Senior engineers cost more than junior sales, for example. Departmental Nuance: Engineering costs might behave differently from Marketing costs. Historical Expenses: Last quarter\u0026rsquo;s expenses are often a solid baseline. Other Cost Drivers: Think office space, IT licenses, whatever else moves the needle. How does it work?\nPretty similar to classification, just with a different goal:\nPrep the Data: Collect historical data connecting your inputs (headcount, department, level, previous expenses) to your output (actual operating expenses). Clean and format it so the algorithm can chew on it. Train the Model: Feed this data to a regression algorithm. Instead of finding a line to separate data points, it tries to find a mathematical equation that best fits the data points. It\u0026rsquo;s looking for a predictable relationship. It might learn that, on average, hiring five new senior engineers correlates with an approximate $10,000 increase in operating expenses. Make Predictions: Now, you can give the model your future hiring plan: \u0026ldquo;We\u0026rsquo;re planning to hire 20 new engineers (split junior/senior) and 8 new junior salespersons next quarter.\u0026rdquo; The model uses the formula it learned to crunch the numbers and outputs a single dollar amount—your predicted operating expenses. Clustering: Finding the Tribes in Your Data So far, we\u0026rsquo;ve been giving the machine an answer key (labeled data). But what if you don\u0026rsquo;t have one? What if you just have a mountain of data and a hunch that there are natural groups hidden inside? This is unsupervised learning, and its most common tool is clustering.\nThe goal is simple: group similar things together. The algorithm figures out what \u0026ldquo;similar\u0026rdquo; means on its own.\nExample: Who Are Your Users, Really?\nA software company wants to understand its user base better. Are they all the same, or are there different \u0026ldquo;tribes\u0026rdquo; of users who behave differently? This helps them tailor features, marketing, and support.\nYou start with raw, unlabeled behavioral data, focusing on metrics like:\nUser ID Logins Per Week Features Used Per Session Session Duration (min) Actions Per Session Cluster (Assigned) U001 7 8 45 25 Power User U002 1 2 10 5 Inactive User U003 4 5 20 15 Intermittent User U004 6 7 40 22 Power User U005 2 3 15 8 Degrading User U006 0 1 5 2 Inactive User U007 3 4 18 12 Intermittent User What features do we need?\nUsage Frequency: How often do they log in? Feature Adoption: How many different parts of the app do they touch? Session Metrics: How long do they stay? How much do they do in a session? Engagement Depth: Are they just logging in, or are they really digging into core functions? How does a Clustering Algorithm Work?\nPrep the Data: Gather all your user behavior data. Remember, no predefined \u0026lsquo;Cluster\u0026rsquo; column here—the algorithm makes those up. We just numericalize our chosen features. Run the Algorithm: You unleash a clustering algorithm (like the popular K-Means) on the data. Imagine it works like this: The algorithm starts by randomly placing \u0026lsquo;k\u0026rsquo; cluster centers (centroids) in your data space. Think of them as initial \u0026ldquo;leaders\u0026rdquo; for groups. It then iteratively assigns each data point (a user) to the nearest centroid. After assignment, it recalculates each centroid\u0026rsquo;s position based on the average of all points assigned to it. The \u0026ldquo;leader\u0026rdquo; moves to the center of its group. This process repeats: users get reassigned to their closest new leader, and leaders move again. This continues until the centroids stabilize, meaning users no longer shift between clusters. Interpret the Clusters: The machine doesn\u0026rsquo;t name the groups; it just creates them. It\u0026rsquo;s up to a human to look at the users in each cluster and give them a meaningful label. You might find: Power Users: High login frequency, extensive feature use, long sessions. Intermittent Users: Moderate usage, sporadic engagement. Degrading Users: Declining usage patterns, fewer features used over time. Inactive Users: Very low or no recent activity. Recommendation Systems: \u0026ldquo;You Might Also Like\u0026hellip;\u0026rdquo; You already know this one. It\u0026rsquo;s the engine behind Amazon, Netflix, and Spotify. Recommendation systems predict what a user might like based on their past behavior and the behavior of similar users. Let\u0026rsquo;s look at two traditional approaches for a streaming service recommending movies.\nApproach 1: Collaborative Filtering (The Wisdom of the Crowds)\nThe core idea: \u0026ldquo;People who liked what you liked also liked\u0026hellip;\u0026rdquo; It doesn\u0026rsquo;t need to know anything about the movies themselves, just who watched what. The data is typically structured as a user-item interaction matrix.\nUser ID Movie A (e.g., \u0026ldquo;Inception\u0026rdquo;) Movie B (e.g., \u0026ldquo;The Dark Knight\u0026rdquo;) Movie C (e.g., \u0026ldquo;Shrek\u0026rdquo;) Movie D (e.g., \u0026ldquo;Parasite\u0026rdquo;) Alice 1 1 0 1 Bob 1 1 0 0 Charlie 0 0 1 0 David 1 0 0 1 Eve 0 0 1 0 (A \u0026lsquo;1\u0026rsquo; indicates the user has watched or liked the movie; \u0026lsquo;0\u0026rsquo; indicates they have not.)\nHow a Collaborative Filtering Algorithm Works:\nUser-Item Matrix Creation: First, we build a giant grid where rows are users and columns are movies. Each cell notes if a user watched/liked a movie. Finding Similarities: The algorithm then hunts for similarities, either between users or between movies: User-Based Similarity: It finds users whose viewing habits closely match the target user. If Bob and Alice watched the same collection of action movies, they\u0026rsquo;re \u0026ldquo;similar.\u0026rdquo; Item-Based Similarity: It focuses on finding movies that are frequently enjoyed by the same group of people. If users who liked \u0026ldquo;Inception\u0026rdquo; also commonly liked \u0026ldquo;The Dark Knight,\u0026rdquo; these two movies are considered similar. Generating Recommendations: User-Based Approach: If Bob watched \u0026ldquo;Inception\u0026rdquo; and \u0026ldquo;The Dark Knight,\u0026rdquo; and Alice watched those plus \u0026ldquo;Parasite,\u0026rdquo; the system might recommend \u0026ldquo;Parasite\u0026rdquo; to Bob because Alice is a \u0026ldquo;taste-twin.\u0026rdquo; Item-Based Approach: If a user watches \u0026ldquo;The Dark Knight,\u0026rdquo; the system looks for other movies commonly watched by people who liked \u0026ldquo;The Dark Knight.\u0026rdquo; If \u0026ldquo;Inception\u0026rdquo; comes up often, it\u0026rsquo;s a good bet for the current user. Training: Algorithms like K-Nearest Neighbors (KNN) or matrix factorization (like SVD) are used to crunch these similarities efficiently. Visualizing the Collaborative Filtering Flow:\ngraph TD A[\"User Interaction Data such as Movie Watch History\"] --\u003e B{\"Create User Item Matrix\"}; B --\u003e C[\"Matrix: Users x Movies\"]; C --\u003e D{\"Calculate Similarity (User User or Item Item)\"}; D --\u003e E[\"Find Similar Users or Items\"]; E --\u003e F{\"Generate Recommendations\"}; F --\u003e G[\"Recommended Movies for User\"]; Approach 2: Content-Based Filtering (If You Like Apples\u0026hellip;)\nThe core idea: \u0026ldquo;You liked this thing, so you\u0026rsquo;ll probably like other things with similar attributes.\u0026rdquo; This method cares deeply about the content of the items themselves.\nMovie ID Title Genre Director Actors (Top 2) M01 Inception Sci-Fi, Thriller C. Nolan L. DiCaprio, E. Page M02 The Dark Knight Action, Crime C. Nolan C. Bale, H. Ledger M03 Shrek Animation, Comedy A. Adamson M. Myers, E. Murphy M04 Parasite Drama, Thriller B. Joon-ho S. Kang, T. Choi M05 Interstellar Sci-Fi, Drama C. Nolan M. McConaughey, A. Hathaway How a Content-Based Filtering Algorithm Works:\nItem Profiling: Each movie gets a detailed \u0026ldquo;profile\u0026rdquo; based on its characteristics—genre, director, actors, even keywords. User Profiling: The system then builds a profile for you, based on the movies you\u0026rsquo;ve liked. If you loved \u0026ldquo;Inception\u0026rdquo; and \u0026ldquo;Interstellar,\u0026rdquo; your profile might scream \u0026ldquo;Sci-Fi,\u0026rdquo; \u0026ldquo;Christopher Nolan,\u0026rdquo; and \u0026ldquo;Thriller/Drama.\u0026rdquo; Recommendation Generation: The algorithm compares your profile to all the unseen movie profiles. It then recommends the movies whose attributes most closely align with your established preferences. For instance, a Sci-Fi fan who loves Christopher Nolan would definitely get \u0026ldquo;Interstellar\u0026rdquo; suggested. Visualizing the Content-Based Filtering Flow:\ngraph TD A[\"Movie Data (Genre, Director, Actors, etc.)\"] --\u003e B{\"Create Item Profiles\"}; B --\u003e C[\"Item Profiles\"]; C --\u003e D[\"User's Liked Items\"]; D --\u003e E{\"Create User Profile\"}; E --\u003e F[\"User Profile\"]; F --\u003e G[\"Compare User Profile with Unseen Item Profiles\"]; G --\u003e H{\"Generate Recommendations\"}; H --\u003e I[\"Recommended Movies\"]; ","permalink":"http://localhost:52022/tech-for-pm/types-of-problems-where-ml-can-be-applied/","summary":"\u003cp\u003eEver wonder how your email app just  \u003cem\u003eknows\u003c/em\u003e  what spam is? Or how Netflix uncannily suggests the exact B-movie you were secretly in the mood for? The answer is Machine Learning (ML).\u003c/p\u003e\n\u003cp\u003eBefore ML, software was a bit of a bureaucrat. You had to write explicit, rigid  \u003ccode\u003eif-this-then-that\u003c/code\u003e  rules for every single possibility. This works fine until the real world, in all its messy glory, shows up. ML flips the script. Instead of feeding a machine rules, you feed it  \u003cem\u003eexamples\u003c/em\u003e. The machine\u0026rsquo;s job is to look at thousands or millions of examples and figure out the patterns on its own.\u003c/p\u003e","title":"Types of problems where ML can be applied"},{"content":"If Westeros taught us anything beyond political murder and dragons, it’s that strategy, storytelling, and ruthless clarity matter, especially when you’re shipping products instead of heirs. Lets check some direct, and maybe sometimes uncomfortable mappings from the iconic Game of Thrones lines to what a product manager actually does and shouldn’t do.\n“Chaos is a ladder.” - Petyr ‘Littlefinger’ Baelish GOT context: Littlefinger watches disorder and uses it as opportunity to climb.\nLesson for PM: When the org, market, or project falls apart, some PMs panic; better PMs diagnose the break, wait for the right time and then step in with a clear path, and become the person who actually delivers stability. You win influence by solving real problems, not by pontificating.\nSpot cascading failures early (data, ops, user complaints). Wait for the right time, let the chaos brew for a while, jumping in too early will not get you the value you demand Propose a focused triage: root cause → temporary mitigation → long-term fix. Communicate clearly: what we fix now, what we delay, who owns it. “When enough people make false promises, words stop meaning anything. Then there are no more answers, only better and better lies.” - Jon Snow GOT context: John Snow when Daenerys tells min that he could have lied to save ass.\nLesson for PM: Commitments do matter, but Over-promising to stakeholders (engineering, sales, execs) burns the team and users. A PM who consistently make promises things just to save his ass in difficult challenging situations, to get out of it easily, then promises, and moreover his words lose value.\n“I am the kind of servant that the realm needs. Incompetence should not be rewarded with blind loyalty. As long as I have my eyes, I\u0026rsquo;ll use them.” - Varys to Daenerys GOT context: Varys to Daenerys, as he takes a stand to value competence and the realm’s wellbeing over blind devotion, when charged by her \u0026ldquo;Proven himself loyal? Quite the opposite. If he dislikes one monarch, he conspires to crown the next one.\u0026rdquo;\nLesson for PM: Stakeholders will often reward loyalty or visibility over effectiveness. The PM’s responsibility is to the users and product outcomes, not to pleasing the loudest sponsor.\nBe the voice of the user realm: surface evidence when leadership wants vanity features. Build alliances across functions - influence, don’t demand. If a stakeholder insists on a harmful direction, propose experiments or guardrails. “Any man who must say, ‘I am the king,’ is no true king.” GOT context: True authority doesn’t require posturing.\nLesson for PM: A PM who shouts titles and demands deference is masking lack of real value. Influence comes from delivering results and enabling others, not from insisting on authority.\nLead by enabling not by shouting and intimidating. Remove blockers, clarify priorities, amplify wins. “Power resides where men believe it resides.” - Varys GOT context: Varys to Tyrion, when Tyrion doubts himself of being powerless.\nLesson for PM: Perceived legitimacy creates power. PMs often have no formal authority over engineering, design, or sales. The “power” of a PM comes from making teams dependent on your product vision, your roadmap clarity, and your ability to remove risk.\nBuild systems and processes that make your role indispensable: clear roadmaps, stakeholder syncs, decision records. Cultivate trust: your estimates and tradeoffs should be reliable. Create shared success metrics that require cross-functional cooperation. “Not today.” - Syrio Forel GOT context: A lesson - we don\u0026rsquo;t just accept death, not today\nLesson for PM: We don\u0026rsquo;t accept impractical requests, not today. Saying “no” is one of the most underestimated PM skills. Not every request need to be taken into the roadmap.\nExplain with a decision framework . Respond with alternatives “There’s nothing more powerful in the world than a good story.” - Tyrion Lannister GOT context: Politics is the narrative. Tyrion knows stories move people.\nLesson for PM: User stories and product narratives sell vision and create alignment. A technically brilliant product without a compelling story will stall adoption and funding. A high potential feature might fail to gain consensus if not presented right.\nTailor the story: execs want ROI and risk-reduction; designers want user empathy; engineers want clear acceptance criteria. Write crisp user scenarios that show the before/after. Storytelling is a habit, practice it in Grooming and you will be able to nail it in the demo. “The storms come and go, the waves crash overhead, the big fish eat the little fish, and I keep on paddling.” - Varys Lesson for PM: Surviving org politics, re-orgs, new leadership, pivots, roadmap resets, and budget winters isn’t about brute force. About 90% of the time things will be turbulent, all around, but as a PM, who is the intersection of it all, you have to ==stand still, endure and keep on progressing quietly.==\n“A mind needs books like a sword needs a whetstone.” - Tyrion Lesson for PM: PMs who stop learning stagnate. Market intuition and product instincts decay without constant sharpening: domain, users, economics, data, and incentive design.\n“If you think this has a happy ending, you haven’t been paying attention.” - Ramsay Bolton Lesson for PM: The next release is not a happy ending. Over-romanticizing launches, pivots, or OKRs leads to disappointment. Real product success involves compromise, casualties, and unglamorous grunt work. A few lessons:\nDon\u0026rsquo;t fall in love with the idea/feature/product, you will be heart broken. Never assume that just this release and things will all be hunky-dory here onwards. A product is a never ending battle ","permalink":"http://localhost:52022/art-of-pm/lessons-in-product-management-from-game-of-thrones/","summary":"\u003cp\u003eIf Westeros taught us anything beyond political murder and dragons, it’s that strategy, storytelling, and ruthless clarity matter, especially when you’re shipping products instead of heirs.\nLets check some direct, and maybe sometimes uncomfortable mappings from the iconic \u003cem\u003eGame of Thrones\u003c/em\u003e lines to what a product manager actually does and shouldn’t do.\u003c/p\u003e\n\u003chr\u003e\n\u003cblockquote\u003e\n\u003ch2 id=\"chaos-is-a-ladder---petyr-littlefinger-baelish\"\u003e“Chaos is a ladder.” - Petyr ‘Littlefinger’ Baelish\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/chaos-is-a-ladder.png\"\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGOT context:\u003c/strong\u003e Littlefinger watches disorder and uses it as opportunity to climb.\u003c/p\u003e","title":"Lessons in Product Management from Game of Thrones"},{"content":"The Evolution of Product Management: A Brief History A little Introduction Product management! The ever-evolving, always-changing job, that is crucial to the success of any product-based company. From its humble beginnings as a behind-the-scenes role to the tech industry\u0026rsquo;s poster child, product management has come a long way. In this post, we\u0026rsquo;ll try to take a deep dive into the evolution of product management.\nProduct management might look like a BS job but not an easy job. It\u0026rsquo;s a lot like herding cats, but instead of cats, you\u0026rsquo;re trying to coordinate a group of developers, designers, and stakeholders to create a product that people will love. So, let\u0026rsquo;s take a look at how product management has evolved over the years.\ntimeline title Evolution of Product Management 1930s : Brand Management emerges at Procter \u0026 Gamble : Full lifecycle ownership mindset begins 1980s : Product role becomes formal in tech-driven companies : User experience slowly starts becoming a priority 1990s : Silicon Valley growth fuels structured PM roles : Customer-first philosophy embraced by innovators (e.g., Apple) Early 2000s : Marty Cagan popularizes modern product thinking : Focus on value creation, empowered product teams 2010s : Lean Startup \u0026 MVP culture (Eric Ries) : Data-driven experimentation and rapid iteration become norms Late 2010s : Product-led growth becomes dominant SaaS strategy : Cross-functional squads and discovery frameworks mature 2020s : Generative AI and automation disrupt PM workflows : PM shifts toward strategic, systems-thinking leadership The Early Days #ProductManagement has been around for a long time, but it wasn\u0026rsquo;t always a formalized role. In the early days of product development, the focus was on engineering and manufacturing, with little thought given to the user experience. As technology advanced and products became more complex, it became clear that there needed to be someone who could oversee the entire process and ensure that the product met the needs of the users.\nEnter the product manager. ==In the 1930s, Procter \u0026amp; Gamble introduced the concept of brand management, which included overseeing the entire life cycle of a product, from development to marketing==. This concept was later expanded to include product management, which focused on the product itself rather than just the brand.\nThe Rise of Silicon Valley Fast forward a few decades, and product management was beginning to take on a more prominent role in the tech industry. In the 1980s and 1990s, Silicon Valley was booming, and product management became a key function in many tech companies.\nOne of the most famous product managers of this era is Steve Jobs, who famously said,\n\u0026ldquo;You\u0026rsquo;ve got to start with the customer experience and work back toward the technology, not the other way around.\u0026rdquo;\nThis philosophy became the foundation of Apple\u0026rsquo;s product development process, which has been emulated by countless companies since.\nAnother key figure in the evolution of product management is Marty Cagan, who was the VP of Products at eBay in the early 2000s. Cagan is the author of \u0026ldquo;Inspired: How to Create Tech Products Customers Love,\u0026rdquo; which has become a bible for many product managers. In the book, Cagan emphasizes the ==importance of focusing on the customer== and creating products that solve real problems.\nThe Lean Startup Movement In recent years, there has been a shift in the way that products are developed. The lean startup movement, which was popularized by Eric Ries in his book \u0026ldquo;The Lean Startup,\u0026rdquo; emphasizes the importance of rapid experimentation and user feedback.\nOne of the key principles of the lean startup approach is the idea of the ==minimum viable product (MVP)==. Instead of spending months or even years developing a product, the lean startup approach involves creating a stripped-down version of the product and getting it into the hands of users as quickly as possible. This allows the product team to gather feedback and make improvements based on real-world usage.\nThe lean startup approach has been embraced by many successful companies, including Dropbox, Airbnb, and Uber.\nThe Future of Product Management (As on before Nov 2022) So, what does the future hold for product management? As technology continues to advance and products become even more complex, the need for skilled product managers will only increase.\nOne trend that we\u0026rsquo;re likely to see in the coming years is the blurring of the lines between product management and other roles, such as design and engineering. As Jeff Gothelf puts it,\n\u0026ldquo;Product management is becoming synonymous with good design and good business.\u0026rdquo;\nIn order to create successful products, it\u0026rsquo;s essential to have a cross-functional team that includes product managers, designers, engineers, and other stakeholders. This means that product managers will need to have a broad range of skills and be able to work collaboratively with others.\nAnother trend that we\u0026rsquo;re likely to see is the ==increased use of data and analytics in product development==. Product managers will need to be skilled at gathering and analyzing data, and using that data to make informed decisions about product development.\nAs David Cancel, CEO of Drift, puts it,\n\u0026ldquo;The product manager of the future will be more data-driven, more customer-focused, and more design-aware.\u0026rdquo;\nIn order to be successful in this role, product managers will need to be adaptable and willing to learn new skills as technology and the market continue to evolve.\nIn comes ChatGPT and other generative AI The impact of ChatGPT and other generative AI tools on the role of product management is not very clear-cut. Some experts believe that AI tools will disrupt product management in a big way, while others argue that it will only enhance the role of product managers.\nAs Mark Faggiano, CEO of TaxJar, puts it,\n\u0026ldquo;The rise of AI and machine learning will likely automate many of the tactical responsibilities of product management, but it will also open up new opportunities for strategic leadership.\u0026rdquo;\nIn other words, AI tools can handle the mundane tasks of product management, such as data analysis and reporting, allowing product managers to focus on higher-level strategic thinking and decision-making.\nOn the other hand, some experts believe that AI tools will eventually replace the need for human product managers altogether.\n\u0026ldquo;AI can predict what the customer wants and needs, what features will have the most impact, and even what the price point should be.\u0026rdquo;\n==If AI tools can do all of that, what\u0026rsquo;s the point of having a human product manager?==\nHowever, many product managers and industry leaders argue that AI tools cannot replace the human touch when it comes to product development. As Teresa Torres, product discovery coach and author, puts it,\n\u0026ldquo;Product management is still a human-centric role, focused on understanding people and solving their problems.\u0026rdquo;\nWhile AI tools can provide valuable insights, they cannot replace the empathy and creativity that human product managers bring to the table, at least not yet.\nSo, while the impact of ChatGPT and other generative AI tools on the role of product management is still up for debate, one thing is clear: product managers will need to continue to evolve and adapt to stay relevant in a rapidly changing technological landscape. As Eric Feng, former CTO of Hulu, puts it,\n\u0026ldquo;Product management will always be necessary, but the definition of the role will change over time.\u0026rdquo;\nAnd as we have seen throughout the brief history, this is, I guess, the fastest evolving role in the entire business world, The key for product managers will be to stay ahead of the curve and ==embrace new technologies as they emerge.==\nProduct management is an essential role and is unlikely to be eliminated any time soon. While the role may evolve with the emergence of new technologies, the need for human product managers who can bring empathy, creativity, and strategic thinking to the table will continue to be in demand. As the future of product management continues to take shape, one thing is for sure: this is an exciting and challenging role that will continue to be in high demand for years to come.\n\u0026ldquo;The role of the product manager is to discover a product that is valuable, usable, and feasible.\u0026rdquo;\nAnd with the right skills and mindset, product managers can keep on doing just that.\n","permalink":"http://localhost:52022/art-of-pm/the-evolution-of-product-management/","summary":"\u003ch1 id=\"the-evolution-of-product-management-a-brief-history\"\u003eThe Evolution of Product Management: A Brief History\u003c/h1\u003e\n\u003ch2 id=\"a-little-introduction\"\u003eA little Introduction\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eProduct management!\u003c/strong\u003e The ever-evolving, always-changing job, that is crucial to the success of any product-based company. From its humble beginnings as a behind-the-scenes role to the tech industry\u0026rsquo;s poster child, product management has come a long way. In this post, we\u0026rsquo;ll try to take a deep dive into the evolution of product management.\u003c/p\u003e\n\u003cp\u003eProduct management might look \u003ca href=\"/art-of-pm/is-product-management-bs-job\"\u003e\n    like a BS job\n\u003c/a\u003e\n but not an easy job. It\u0026rsquo;s a lot like herding cats, but instead of cats, you\u0026rsquo;re trying to coordinate a group of developers, designers, and stakeholders to create a product that people will love. So, let\u0026rsquo;s take a look at how product management has evolved over the years.\u003c/p\u003e","title":"The Evolution of Product Management"},{"content":"AI-Powered Email Processing System for a Fashion Retailer Turning chaotic inboxes into structured, automated order \u0026amp; inquiry workflows Retail operations look deceptively simple from the outside: customers email, staff respond, orders get booked, stock gets updated. Under the hood, it’s usually an unstructured mess—especially for small or mid-size fashion brands that haven’t fully automated their digital operations. The result is slow responses, lost orders, inconsistent inventory updates, and frustrated customers.\nThis project is a practical proof-of-concept that shows how LLMs can be deployed as real workers in this workflow. Not abstract magic. Actual operational logic.\nThe goal: Automatically read incoming emails, understand whether they are order requests or product inquiries, extract structured data, check stock, update inventory, and generate responses—end to end.\nEverything runs through a modular chain of LLM-based “micro-agents,” each responsible for a small, verifiable task. This keeps the system interpretable instead of becoming a mysterious black box.\nGithub Repo: Business Context: Why This Problem Exists A fashion retailer receives two types of recurring emails:\n“Do you have this in size M?”\n— product inquiries\n“I want to order 2 beige linen shirts.”\n— order requests\nThese land in a common inbox. Humans spend hours manually:\nreading each email\ndetermining intent\nchecking product catalog and stock\npreparing responses\nadjusting inventory\nlogging orders manually in sheets / db\nThe friction multiplies with scale.\nThe mistakes multiply even faster.\nAn LLM-based system solves exactly these bottlenecks:\nno missed orders\nreal-time stock updates\nconsistent responses\nstructured logs for auditing\nscalability without hiring more staff\nThe system is not meant to be a chatbot. It is meant to be an intelligent back-office worker—fast, predictable, and fully auditable.\nTechnical Overview: How the System Works The design intentionally avoids monolithic “one giant prompt” architectures. Instead, each function is a narrow expert—an independent agent.\nProcessing happens sequentially, not in batches, to maintain inventory correctness. When two emails request the same product, sequential order ensures one stock update happens before the next evaluation.\nEverything flows through a deterministic pipeline:\nflowchart TD A[llm_classify_email] --\u003e B[email-classification log] B --\u003e C{Is Order Request?} %% Order Request Path C --\u003e|Yes| D[llm_extract_order_items] D --\u003e E[process_order_requests_pipeline] E --\u003e F[llm_generate_order_response] F --\u003e G[order-response log] %% Product Inquiry Path C --\u003e|No, Product Inquiry| H[answer_inquiry_with_stock - RAG and stock lookup] H --\u003e I[inquiry-response log] %% Final Step G --\u003e J[Write all logs] I --\u003e J[Write all logs] This structure makes the system easy to reason about, easy to debug, and easy to extend.\nPotential Future Extensions Multi-store inventory sync If stock is distributed across warehouses. Price negotiation automation Common in boutique fashion retail. CRM integration Tag customers by intent, lifecycle, and purchase history. Predictive insights Incoming email patterns → demand forecasting (email-driven signals are underutilized gold) Full order booking Push confirmed orders directly into ERP or Shopify. The pipeline already supports extension because each agent is isolated.\n","permalink":"http://localhost:52022/projects/ai-powered-email-processing-system-for-a-fashion-retailer/","summary":"Turning chaotic inboxes into structured, automated order \u0026amp; inquiry workflows","title":"AI-Powered Email Processing System for a Fashion Retailer"},{"content":"The project enables chatting and extracting information and insights from internal documents using a locally deployed LLM.\nKey Features\nUses Retrieval-Augmented Generation (RAG) and LangChain techniques Supports various document formats Provides accurate and context-aware responses based on document content 💡 Goal: Facilitate efficient and intelligent information retrieval from internal documents.\nTechnologies Used: LLM, RAG, LangChain, Streamlit, Ollama\n🔗 GitHub 📝 Read on LinkedIn ","permalink":"http://localhost:52022/projects/chat-with-local-documents/","summary":"Enables intelligent chat and data extraction from internal documents using a locally deployed LLM with RAG and LangChain.","title":"Chat with Local Documents"},{"content":"The quickest and cleanest possible way\nMany teams today want to let non-technical users query data using plain English instead of writing SQL or dashboards. I have been playing around with multiple frameworks and techniques to achieve this, and finally I landed upon open ai agenst sdk + motherduck data warehouse, this is the quickest, cleanest, effective and at the same time thoroughly customizable way I could find.\nCombining MotherDuck’s data-warehousing power with agents that can generate and run SQL makes that possible. In this post I walk you through a minimal end-to-end example — how to wire up a simple agent that queries a database on MotherDuck.\nWhy use MotherDuck + OpenAI Agents SDK Before diving into code, a quick look at why this combo makes sense:\nMotherDuck’s strengths MotherDuck gives you per-user (or per-tenant) isolation: each user (or customer) gets their own dedicated DuckDB instance (“duckling”), so analytics workloads don’t interfere with each other. That’s great for multi-tenant setups or customer-facing analytics. Because DuckDB is lightweight and embeddable, you get really low latency. You don’t need a heavyweight MPP cluster. That makes queries fast enough for interactive use. MotherDuck supports “AI + database” workflows: agents connected to MotherDuck can generate SQL, run queries, and fetch results — bridging LLMs and structured data elegantly. It’s free tier supports personal use, good for POC and experimentations OpenAI Agents SDK’s strengths The SDK provides a minimal set of primitives (agents, tools, sessions, guardrails) that make it easy to build agentic apps. It lets you wrap any Python function as a “tool”: that means you can write a function to execute SQL (against MotherDuck), expose it as a tool, and let the agent call it dynamically. Because it’s Python-first and lightweight, you can prototype quickly without massive boilerplate. Put together, this architecture lets you build natural-language analytics agents that query real data stored in a scalable, isolated, low-latency warehouse.\nHigh-level Flow Get access (account + token) to MotherDuck and have a database loaded (or use the sample DB tables for experimentation like I will demonstrate in the sample code). In Python, wrap a function that connects to MotherDuck (via DuckDB) and runs arbitrary SQL. Use OpenAI Agents SDK to define an agent, expose your SQL runner as a tool. Send natural language questions to the agent. The agent generates SQL, executes via the tool, returns results. Below is the sample code illustrating this flow.\n# sample_agent.py from agents import Agent, Runner, function_tool import duckdb # Tool: runs SQL against MotherDuck @function_tool def run_sql(query: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Execute SQL query against MotherDuck and return results as string. \u0026#34;\u0026#34;\u0026#34; token = os.getenv(\u0026#34;MOTHERDUCK_TOKEN\u0026#34;) conn = duckdb.connect(f\u0026#34;md:my_database?motherduck_token={token}\u0026#34;, read_only=True) try: df = conn.execute(query).fetchdf() return df.to_string(index=False) except Exception as e: return f\u0026#34;Error executing query: {e}\u0026#34; # Define the agent agent = Agent( name=\u0026#34;AnalyticsAgent\u0026#34;, instructions=( \u0026#34;You are a data assistant. \u0026#34; \u0026#34;When user asks a question about data, generate valid DuckDB SQL, \u0026#34; \u0026#34;execute via run_sql tool, and return the result.\u0026#34; ), tools=[run_sql], # optionally configure model / temperature here ) def ask(question: str): result = Runner.run_sync(agent, question) return result.final_output if __name__ == \u0026#34;__main__\u0026#34;: user_query = input(\u0026#34;Ask something about data -\u0026gt; \u0026#34;) print(ask(user_query)) run_sql: a Python function decorated as a “tool” so the agent can call it whenever it needs to run SQL. It connects to your MotherDuck database (via DuckDB), executes the SQL, returns results. Agent(...): creates an agent with a system prompt (instructions) telling it what its role is. We give it access to the run_sql tool. Runner.run_sync(agent, ...): sends a user question, triggering the agent loop: the agent uses the LLM to generate SQL + reasoning, then calls run_sql, grabs results, returns them. With this minimal setup, you already get a natural-language → structured-data interface.\nFor a full working code with a little enhancements such as adding a schema definition, query guidance which makes the agent response time shorter, crisper and cleaner, checkout the repo here:\n[GitHub]]( https://github.com/dibyendutapadar/simple-data-analytics-motherduck-openai-agents )\n","permalink":"http://localhost:52022/projects/simple-analytics-agent-motherduck-openai-agents-sdk/","summary":"Building a Simple Analytics Agent with MotherDuck + OpenAI Agents SDK","title":"Building a Simple Analytics Agent with MotherDuck + OpenAI Agents SDK"},{"content":"Part 1 of \u0026ldquo;AI ain\u0026rsquo;t Magic, It\u0026rsquo;s Math\u0026rdquo; Before AI became what it is today, machines behaved like bureaucrats: they only did what you explicitly told them to do. Software was ==Rules Based==. Meaning: you wrote conditions, and the machine reacted to those conditions.\nFirst you create the rules mentally\nIf A happens then X, If B and C happends then do Y, If A and D and E Happens then Do Z Then hard code it\nif A: if D and E: execute(z) else: execute(x) elif B and C: execute(y) It works. It\u0026rsquo;s very deterministic. But only until the real world becomes messy, and with the scale of data we deal with today, in certain cases, this approach couldn\u0026rsquo;t fly.\nLets take the example of lead scoring in sales. A company have a dataset of\nSource Company_Size Industry Budget \u0026hellip; Final_Score inbound 1200 SaaS 20000 \u0026hellip; High inbound 800 Retail 10000 \u0026hellip; High outbound 6000 Finance 15000 \u0026hellip; Medium inbound 200 Travel 3000 \u0026hellip; Low referral 500 SaaS 50000 \u0026hellip; High event 350 Retail 8000 \u0026hellip; Medium outbound 1000 Healthcare 25000 \u0026hellip; Medium inbound 1500 Finance 60000 \u0026hellip; High event 200 Education 2000 \u0026hellip; Low outbound 400 SaaS 5000 \u0026hellip; Low At first, companies write a few rules based on observations:\ninbound + large company + SaaS → High\noutbound + small company + low engagement → Low\nreferrals → High\nevent + booth + mid-size → Medium\nThese patterns come from looking at past leads and noticing what usually works.\nSomething like this:\nflowchart LR A[inbound + large company + SaaS] --\u003e H1[High] B[outbound + small company + low engagement] --\u003e L1[Low] C[referrals] --\u003e H2[High] D[event + booth + mid-size] --\u003e M1[Medium] The problem?\nAs business grows, the number of conditions explodes.\nYou add exceptions, sub-rules, and patches.\nEventually, it becomes a jungle of “just add one more condition” hacks.\nMaintaining this is not a job, it\u0026rsquo;s a punishment.\nThis is the limit of the rules-based world.\nThe Shift: from writing rules to learning from examples Instead of telling the machine:\n“If X and Y and Z happen, give High score…”\nwe give it all past examples:\nSource | Company_Size | Industry | Budget | ... | Final_Score inbound | 1200 | SaaS | 20000 | ... | High outbound | 6000 | Finance | 15000 | ... | Medium referral | 500 | SaaS | 50000 | ... | High event | 200 | Education | 2000 | ... | Low ... Everything except Final_Score is the input (features).\nFinal_Score is the output (target).\nThe machine’s job:\nLook at thousands of “input → output” pairs and learn the patterns humans used to manually write.\nThat’s it.\nNo magic.\nJust pattern learning at scale.\nOnce the machine has learned from the old data, give it new unseen leads:\nSource: inbound Company_Size: 1800 Industry: SaaS Budget: 45000 Past_Engagement: high ... And it will say:\n→ High\nNo one wrote this rule.\nIt emerged from patterns in the data.\nHumans can’t do this reliably when the dataset crosses a few thousand rows.\nMachines thrive on it.\nMore data → more nuance → better patterns.\nThis is the essence of Machine Learning:\nWhere on one side learning from huge data set is cognitively impossible for a human brain to create the rules, that\u0026rsquo;s a boon for AI - more the data, more the nuances and edges captured and learnt.\nThat shift from if-else rules to learning from examples is the intuitive jump from traditional software to modern ML.\nThe idea looks solid, but now the task was to create algorithms on how machine could learn from data More about that in the next part here\nWhat all can ML do ","permalink":"http://localhost:52022/tech-for-pm/an-intuition-for-machine-learning/","summary":"\u003ch3 id=\"part-1-of-ai-aint-magic-its-math\"\u003ePart 1 of \u0026ldquo;AI ain\u0026rsquo;t Magic, It\u0026rsquo;s Math\u0026rdquo;\u003c/h3\u003e\n\u003cp\u003eBefore AI became what it is today, machines behaved like bureaucrats: they only did what you explicitly told them to do.\nSoftware was ==Rules Based==.\nMeaning: you wrote conditions, and the machine reacted to those conditions.\u003c/p\u003e\n\u003cp\u003eFirst you create the rules mentally\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIf  A happens then  X, If B and C happends then do Y, If A and D and E Happens then Do Z\nThen hard code it\u003c/p\u003e","title":"An Intuition for Machine Learning"},{"content":"..coming soon\n","permalink":"http://localhost:52022/tech-for-pm/brief-history-of-ai/","summary":"\u003cp\u003e..coming soon\u003c/p\u003e","title":"Brief History of AI"},{"content":"The word ‘Strategy’ in the business world has become almost synonymous with ‘Religion’ in general. Everyone claims they have one, everyone says their one is the right one, but very few understand what it is and a lot keep defining it in the most complex and contradictory ways.\nOne of our professors used to say,\nStrategy is the most used and least understood word in the business world.\nJust try to recall, how many times were you hurt to see this poor word, being beaten to death in those coveted meeting rooms.\nThere is a tendency of Product teams to enact themselves as the entire business (PMs love to call themselves mini CEOs 😛), and there\u0026rsquo;s no reason they would stay away from enacting this behavior as well, and that did happen. It didn’t take long before ==‘Product Strategy’ became a buzzword and just like in business, it stayed grossly overused and tremendously less understood.==\nWhat Strategy is not and what it is? In the overall Business sense, there is a tendency to confuse Operational excellence as Strategy. Operational effectiveness is not a strategy. The plan to reduce inefficiencies in your business is not a strategy.\nSimilarly, in a product sense, A Roadmap is not a Product Strategy. Having a sequence of tasks to deliver is not a strategy. More into that later, let us begin from the basics. What is Strategy?\nStrategy = A set of choices\nThat’s how Roger Martin defines it in the book, Playing to Win: How Strategy really works, and that’s it; it\u0026rsquo;s that simple. Strategies are choices to do something, to be something, etc. What we fail to comprehend is that every time we make a choice to do or be something, we are also making a decision NOT to be or do something. If we are NOT saying NO to a choice, it\u0026rsquo;s not a strategy, it\u0026rsquo;s just obvious.\nA very simple litmus test to check if we have a strategy is to ask ourselves, ==did we say NO to an alternative while choosing the other?== and why so?\n\u0026ldquo;The essence of strategy is choosing what not to do.\u0026rdquo; — Michael E. Porter\nStrategy by examples It’s very difficult to comprehend such a widely overused word. So let’s put our bet on examples.\nIt’s not true that if you have a strategy, i.e, you have made certain choices, you will definitely succeed. More often than not, you will fail (That\u0026rsquo;s what the data says, About 90% of Businesses fail, and they all had a strategy)\nLet\u0026rsquo;s have a look at both sides. First, two success stories\nTesla: Conventional business logic is that when you\u0026rsquo;re starting something new, you create a \u0026lsquo;Minimal Viable Product\u0026rsquo; or MVP and sell it at a reasonably low starting price, or go freemium to get the initial growth. But Tesla decided not to do that and play the long-term game. Instead, Tesla created the most luxurious, expensive, fully-featured sports car they could afford. That car was the Tesla Roadster, and for context, the newest generation of the Roadster will retail from upwards of US$200,000 for the base model. This was the first car they ever produced - knowing that they couldn\u0026rsquo;t achieve the necessary scale or efficiency to turn a profit (even at such a high price). However, such a car was in-line with Tesla’s vision statement where they aim “to create the most compelling car company of the 21st century by driving the world’s transition to electric vehicles.” AirBnB: They Chose to improve the quality of every listing and said no to the volume of the listing. In a more basic sense, it opposes one of the most commonly stated principles of building a tech startup - “everything must be scalable”. The co-founders grabbed their cameras and visited every one of their NYC listings. They persuaded the owners to let them take a ton of photographs of their places. This is anything but scalable. Later, they did scale their initial solution by hiring young photographers in major locations and paying them to take professional photos of owners’ listings (at no charge to the owner). And in contrast to Tesla, Airbnb\u0026rsquo;s story shows that business strategies don’t have to be grand and super long-term affairs. Now two failure stories.\nKodak: It is well-known and well-quoted how Kodak invented the Digital camera, but decided not to launch it. This was their strategy, though a failed one. Kodak chose to NOT hurt the selling of single-use rolls of films by launching a Digital. Even when they were told that they had at most, 10 years until digital would completely displace film - they continued to resist in order to ensure that they met their own short term financial KPIs. It was a tough decision to take. Sabotaging own product takes courage, and they didn’t choose to do that Blackberry: Blackberry\u0026rsquo;s revenues were firmly entrenched in the B2B business model. They sold to corporations who then issued phones to their users. Blackberry\u0026rsquo;s entire pitch was geared towards corporations and they resisted user trends such as touch screens and mobile games because businesses didn\u0026rsquo;t seem to want these things. They chose to cater to the corporations and not the end users. That’s the choice they made, and that was their strategy. And when Apple came in, they were thrown out of the market. Circling back to where we started. If you think you have a strategy, just ask, did you say NO to an alternative or you are just doing everything that\u0026rsquo;s coming your way?\nProduct Roadmap is not Product Strategy The most common mistake that product managers often do is to confuse Product Strategy with a product roadmap, or a product plan. Strategy plays in the realm of the unknown, plans on the other hand, try to bring certainty and order to the chaos.\nConfusing the two, ‘strategy’ and ‘planning’ is a common mistake. A plan is quite simple. It is the detailed steps of how you are going to achieve something. Your roadmap is a form of plan. Your sprint plan, the way you intend to perform a release, etc are all plans.\nAs Mike Tyson famously said, “Everybody has a plan until they get punched in the mouth”…\nThink about strategy as how you’re going to deal with being punched in the mouth — are you going to punch back, kick, protect yourself, run away? What are the set of key choices that will guide future decisions? Are you a fighter or no? Do you want to be known as one or no?\nWho makes the Strategy? There is no magical line in the org where strategy begins. The “decision-makers” and “executors” dichotomy is a fallacy. Strategy is more of a spectrum. Everyone in the organization makes a set of choices, just at varying levels. The problem is, we treat bigger choices as strategies and smaller choices as not.\nFor example, making a decision on whether the company should acquire another company - that’s a pretty big choice. As opposed to making a decision to write a line of code a certain way or to put a button in a specific spot on the screen. These are still choices. Just strategies on a smaller scale with a smaller impact.\nSo the notion, that as you become senior you become more strategic, is not entirely true. Rather as a senior, you are now responsible to make bigger decisions, with bigger impact and hence tougher choices. You’re just operating at a different scale — like going from playing the guitar at home alone to playing at a professional level in front of a live audience as part of the symphony orchestra.\nHow to Build your Product Strategy: There are a lot of frameworks lying around the internet, the Amazon 6 pager, the Lean Canvas, Netflix DHM model. It’s very tempting to pick up a framework to build a strategy, but just like the same strategy doesn’t work for all, the same framework won’t work for all.\n(Personal Opinion) It’s always better to craft your own strategy framework, because your case is different from all other business cases. If it had already been done by someone else, why would you even do it?\nLet\u0026rsquo;s try to decode and conquer the 4 basic steps of making a strategy.\nWhat do you want to achieve? What’s stopping you from achieving them? What are your choices? Which choice are you going to make? Step 1: What do you want to achieve? - The Vision Before jumping to the strategy you need a Product Vision. This is often guided by the business vision. Let\u0026rsquo;s assume that the business has already made a choice to be something and NOT to be something, and a desired state for the Product is defined. Thereafter, it comes down to the Product to have their strategy around it.\nExamples of product vision:\nIn 10 years, Uber will be, the cheaper alternative to owning a car or to taking public transport\nIn 5 years Google Workspace will be the default choice to create, communicate, and collaborate where work is more flexible, time is more precious, and enabling stronger human\nIn 4 years, Amazon will be a place where people can come to find and discover anything they might want to buy online.\nIt\u0026rsquo;s often beneficial to attach some metric to the product vision, which makes it easier to define the problem space. That being said, there might be cases when you are too far away from defining metric, in those cases, forcefully superimposing a metric just for the sake of it is not recommended.\nStep 2: What’s stopping us from achieving the Vision- The Problem Space. Once you have the vision in place, the next part is to examine what major challenges are there in the way to achieving them. The challenge can be anything and in any space, for Uber, it can be the waiting time, the cost, and the availability of cars/drivers. For Google Workspace, it can be their competitor (Microsoft). For Amazon, it can be traffic, listing spread and quality, and availability of sellers.\nThe Problem spaces can be multifold. You do not need to focus on just one here. You can, and most probably you will, have to prioritize; which one will you be solving first. That being said but you do need to know and be mindful of the entire problem space.\nStep 3: What are my choices? Making choices are difficult. So we should start from the basics. Just like you need to know all your problem statements, you need to know all the alternative choices that may help to solve the problem spaces. And then you make a set of choices from them.\nHow to list down the choices? A basic set of exploration in all your control areas can help you get the list of choices.\nDisclaimer: This can be either relevant or irrelevant for individual cases, these are just examples and not a framework.\nState of the product: At what stage the product is currently at? Can I continue like this to achieve the vision or do I need a revamp? If I need a revamp, do I need to do it now, or can I delay it, and by how much? What the data says: What are my metrics? Are my metrics aligned with my vision? If not, what do I need to change? Are the current data points directing that we can achieve the vision? Which data points should I focus on and which data points should I not? Which features will help me set the data right and which feature should will not? What the customers say: Do the customer feedbacks resonate with my vision? Am I providing the Product which will make customers spell out the Vision by themselves? If not which feature should I build and which feature should I NOT build? What the Stakeholders want: Do the features resonate with both stakeholders and the vision? If not, which should I discontinue and which should I choose to prioritize? What the competitors are doing: Is my strategy the same as my competitors? If my vision is different from my competitor, then how can I have the same strategy in the same market? What should I NOT do that is currently aligning with my competitors\u0026rsquo; vision and not my vision? Step 4: Making the choice This is the most difficult part. Unlike what most people claim, there’s no right or wrong, there’s no framework or formula for success. If there would have been, then all business would have succeeded. Business history is full of examples where a similar strategy has been a success for one org and a disaster for another. It’s completely up to the concerned people’s acumens, experiences, biases, passions, business and logical senses, knowledge etc, to take the decision.\n\u0026ldquo;The real challenge in crafting strategy, lies in detecting subtle discontinuities that may undermine a business in the future. And for that there is no technique, no program, just a sharp mind in touch with the situation.\u0026rdquo; — Henry Mintzberg\nIt’s important to have a strategy, that is to ==have a set of choices, chosen from a list of alternatives.== Having a strategy doesn’t guarantee success, but not having a strategy almost always guarantees failure, and sometimes it’s delayed. A delayed failure is worse than a quick failure. In case of a quick failure, we will still have the chance to change the strategy.\nHowever, making a strategy takes a tremendous amount of courage; ==Courage to say NO, Courage to be held accountable for the outcome, and that’s not easy==. The best Product Leaders are the ones who made choices.\nLet\u0026rsquo;s have a look at an example, A strategy that Sundar Pichai (In my opinion one of the greatest Product Leaders) undertook took when he started at Google.\nSundar Pichai and the story of Chrome Pichai started at Google, leading product management for the Google toolbar, a critically strategic product (as per the management at that time) that enabled default search queries on different web browsers to go through Google and allow them to track browsing behavior to power the AdWords targeting engine. At the time, Internet Explorer was the “installed by default” incumbent for many users, while Firefox was the alternative browser of choice.\nIn 2006, Pichai recognized that having to add a toolbar to your browser to perform a basic function like ‘search’ might NOT be the best strategy. The first time Pichai pitched the idea for Chrome, it was shot down for being \u0026rsquo;too expensive\u0026rsquo;. It was only after Pichai compiled data and bid his case to the stakeholders, that the project was greenlit. Now, Chrome is the most popular search engine across platforms and devices (not safe and secure, but the most popular)\nPichai identified a weakness in Google’s strategy, and Chrome began as a defensive play against the established browsers to protect and grow Google’s search business (which still generates much of the company’s revenue). There was no guarantee other browsers would continue making it easy for toolbars like Google’s to be installed by users, particularly Microsoft, who was still hoping they could establish their own inroads in the search market with Bing.\nChrome wasn’t just another alternative web browser, it was a foothold on the desktop extending Google’s reach into the consumer web experience. Google was no longer just the homepage when you launched your browser or the results that came up when you entered a search query; it was now an end-to-end browsing experience.\nNow that is a Product Strategy, that quarterly/half-yearly roadmap on a slide deck is not. ","permalink":"http://localhost:52022/art-of-pm/do-you-have-a-product-strategy/","summary":"\u003cp\u003eThe word ‘Strategy’ in the business world has become almost synonymous with ‘Religion’ in general. Everyone claims they have one, everyone says their one is the right one, but very few understand what it is and a lot keep defining it in the most complex and contradictory ways.\u003c/p\u003e\n\u003cp\u003eOne of our professors used to say,\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eStrategy is the most used and least understood word in the business world\u003c/em\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eJust try to recall, how many times were you hurt to see this poor word, being beaten to death in those coveted meeting rooms.\u003c/p\u003e","title":"Do you have a Product Strategy?"},{"content":"TLDR; The above infographic from the blog output vs outcome vs impact by Christophe Achouiantz does a great job of explaining the three terms in one shot. That’s all there is (literally ☺️), but if you want to explore a bit more with examples, please read on and leave a comment.\nA Mandatory Introduction No matter how long you’re already working in product management you must have heard some “product leader” discussing:\nThe great impacts the team will create How much do outcomes matter in the organization How you can’t stop being focused on delivering output or something similar.\nIf you are (or have been) in Product, you must certainly have been part of a lot of meeting rooms, where those terms have had their fair share of talking time.\nBased on the context of one’s company or industry, one creates his/her own interpretation of these terms, and they might have a different meaning based on their company’s culture or the way of working\nThat being said, I feel that there’s so much interchangeable views of these terms out there, that their actual meaning gets lost very often. But,\nIt would be a shame if the use of these terms is done in the regard, that someone just loves to sound smart or wants to blow a discussion out of proportion by using these jargons, when they have nothing solid to contribute.\nFrustrations apart ( 😂), a much worse case of that is, the wrong understanding of those terms, which leads to misaligned and even misdirected product decisions.\nLet’s try to take a look at these terms and make an effort at demystifying them.\nIMPACT We will be starting with impact, because it’s the one that typically sits at the highest place in the hierarchy of these terms.\nTo me,\nan impact describes a high level expression of where a company wants to be, expressed through result or metric.\nSome of the main characteristics we can see when we look at an impact is that, it typically is something that affects the whole company. Which can, in turn, be changed or impacted through many initiatives coming together.\nSome instances can be company wide revenue; customer satisfaction, measured for example through customer effort score; or specific user activity metrics like weekly active users or certain other behavior which represents activity for your users.\nImpact typically expresses one of three categories either it looks at something qualitative, something quantitative, or around a metric which describes the efficiency in the company.\nExample: In our examples, let’s chose an organization and stick to it for better context. Let’s pick an EdTech org, and name it ‘Smart Tutor’ or something. (Really not good at picking names 🙈).\nApart from online content, and live classes, Smart Tutor goes beyond objective assessments, and enables AI-driven automatic scoring and grading of subjective assessments, which learners can type or even write in pen and paper mode.\nIn the context of Smart Tutor, the impact used can be the one, which describes their focus, on the number of active learners and/or the amount of active time learners spend on the platform. This obviously makes a lot of sense for a learning platform because it describes the health of the company and allows them to project where the company wants to be in terms of core product usage. It’s also a metric, where many departments, teams, and initiatives have to contribute, in order to actually change it.\nSo far so good, but the important thing lots of people oftentimes forget is that, the primary impact for a company, or for a team, can change overtime. This might be based on seasonality, revenue, customer experience, or some weird external factors.\nGoing back to the Smart Tutor, there might be a time in history, where they might suddenly have a huge increase of customer issues and support requests due to some technical problems or as simple as a half-baked product, where they built an MVP but forgot to convert it to an FVP (Fully Viable Product).\nIn this case, for a specific period of time, the company has to switch the primary impact to reducing customer complaints. Consequently, this switch should guide which initiatives to pursue. Otherwise, the whole growth anatomy of the company can go for a toss. If they decide to ignore this and still keep focusing on push marketing and sales to increase learners, very soon the impact will start crumbling, in spite of incurring massive costs.\nImpacts should not be kept static. Even if they are carved on rocks, it’s high time to break the rock, else, in no time, the rock will become a boulder and break the org.\nOUTCOME Next up is outcome. This is probably the one term product teams love to use the most these days and what everybody is talking about now, is the definition.\nTo me,\nAn outcome typically describes a measurable change in behavior that contributes to an impact, meaning you should not focus on an outcome if it doesn’t contribute to a company wide metric.\nThe wide popularity of the term outcome is certainly influenced by Josh Seiden and his writing in this great book outcomes of outputs.\nNow this change in behavior, is really something that can affect anyone, whether it’s your users your customers, or internal stakeholders. Outcomes are relevant for product teams for two main reasons;\nImpacts might be too detached from a single product or feature release which makes it very hard for product teams to actually measure the success of their efforts. Outcomes can serve as a link between features and business metrics to serve as some kind of proxy metric so that teams can measure their progress and success more easily.\nStarting a conversation around prioritization based on outcomes leaves way more room for creativity for the team, to explore which solutions are actually most likely to achieve.\nOutcomes should describe for whom you want to create the change behavior and to what extent the behavior has to change.\nExample Let’s go back to Smart Tutor. For them, an outcome could be “Enabling the system to share assessment results with the learners within 1 min”. Here, we are specifically naming the type of user we’re trying to change their behavior for and to what extent. This context is important, because if we use outcomes as a frame for running ideations sessions, we can come up with proper solutions to create this particular change in behavior.\nIf we can be very clear and specific about whether it’s about making something faster, more convenient, or more accessible. This is pretty important to provide our team members and participants of the ideation sessions beforehand, to give them enough context, so that they can think of the best possible solutions in terms of the outcome.\nAlso, when we want to use outcomes for goal setting, for example by using OKRs, those outcomes should be turned into a specific number that we can measure.\nOUTPUTS Now Speaking of solutions let’s also look at outputs.\nOutputs are essentially the artifacts product team delivers through activities like scrum sprints or Kanban cycles.\nIt describes a specific product or specific feature; and while many discussions revolve now in the industry on how they are prioritizing outcomes over outputs, outputs are still pretty important because it doesn’t really matter how great your selection of outcomes is, if you’re not providing a tangible solution in the form of an output for your users, the behavior of your target audience will probably not change.\nExample: Let’s go back to ‘SmartTutor’. To drive the mentioned outcome “Enabling the system to share assessment results with the learners within 1 min”, The outputs can be to deliver an epic to read handwritings with 95% accuracy within 6 sprints or it can be to deliver a feature which reduces the time to upload handwritten pages from 20 seconds to 5 seconds.\n%%{init: { \"themeVariables\": { \"fontSize\": \"10px\" }, \"flowchart\": { \"nodeSpacing\": 4, \"rankSpacing\": 40 } }}%% flowchart TB %% --- Color Classes --- classDef impact fill:#FFF4D6,stroke:#E6B656,color:#4A3B09; classDef outcome fill:#DFF1FF,stroke:#72A8D8,color:#0E3A5F; classDef output fill:#E8F9E8,stroke:#54A66A,color:#1E4621; subgraph IMPACT I1[\"Increase Active Learner Engagement\"] class I1 impact; end subgraph OUTCOMES direction TB O1[\"Results delivered \u003c 1 min\"] O2[\"Fewer assessment-related complaints\"] class O1,O2 outcome; end subgraph OUTPUTS direction TB P1[\"OCR handwriting engine (95%)\"] P2[\"Upload pipeline: 20s → 5s\"] P3[\"Async grading + queueing\"] P4[\"Real-time alerts\"] class P1,P2,P3,P4 output; end I1 --\u003e O1 I1 --\u003e O2 O1 --\u003e P1 O1 --\u003e P2 O1 --\u003e P3 O1 --\u003e P4 O2 --\u003e P2 O2 --\u003e P3 %% Optional smaller text styling style I1 text-size:12px style O1 text-size:11px style O2 text-size:11px style P1 text-size:10px style P2 text-size:10px style P3 text-size:10px style P4 text-size:10px If you are reading this and made it so far, please share this in the meeting rooms where you see these 3 jargon being overused to no fruitful context. Let\u0026rsquo;s make meetings less painful and more fruitful 😊.\nInspirations: Book: Outcome over Outputs by Josh Seiden Podcast: Product Thoughts hosted by Tim Herbig ","permalink":"http://localhost:52022/art-of-pm/impact-outcome-output/","summary":"\u003ch3 id=\"tldr\"\u003eTLDR;\u003c/h3\u003e\n\u003cp\u003eThe above infographic from the blog \u003ca href=\"https://blog.crisp.se/2019/10/16/christopheachouiantz/output-vs-outcome-vs-impact\"target=\"_blank\" rel=\"noopener\"\u003e\n    \u003cem\u003eoutput vs outcome vs impact\u003c/em\u003e\n\u003c/a\u003e\n by Christophe Achouiantz does a great job of explaining the three terms in one shot. That’s all there is (literally ☺️), but if you want to explore a bit more with examples, please read on and leave a comment.\u003c/p\u003e\n\u003ch2 id=\"a-mandatory-introduction\"\u003eA Mandatory Introduction\u003c/h2\u003e\n\u003cp\u003eNo matter how long you’re already working in product management you must have heard some “product leader” discussing:\u003c/p\u003e","title":"IMPACT, OUTCOME, OUTPUT - A use case"},{"content":"You might already be aware of the widely discussed 2013 article On the Phenomenon of Bullshit Jobs: A Work Rant by David Graeber The major points (and why they are even more relevant in the age of AI are):\nTechnological progress made it ==possible to reduce working hours== (as Keynes predicted), yet societies have not adopted it. Instead of freeing time, systems manufactured more jobs. When John Maynard Keynes Predicted a 15-Hour Workweek “in a Hundred Year’s Time” (1930) Productive and manual jobs have ==declined because of automation== (and now because of AI thats declining even faster), but white-collar bureaucratic and pseudo-service roles have exploded. Many workers privately believe ==their work has no real societal value==, creating psychological harm, resentment, and existential dissatisfaction. The system benefits from people staying busy, disciplined, and dependent. Idle citizens with free time are viewed as politically dangerous, so ==employment becomes a moral obligation.== Society perversely ==rewards low-value or exploitative roles more than essential ones== (nurses, teachers, sanitation workers). This fuels resentment against people with meaningful jobs, not meaningless ones. He later published a book on the same topic, and outlined 5 major types of BS jobs\nFlunkies: Jobs that exist mainly to make someone else look or feel important. Goons: Roles that exist only because rivals have similar roles, essentially arms-race jobs. Duct Tapers: Employees hired to temporarily fix structural problems that shouldn’t exist in the first place. Box Tickers : People hired so organisations can claim compliance, progress, innovation, or efficiency that they aren’t genuinely delivering. Taskmasters Unnecessary managers hired to oversee people who don’t need oversight. People whose job is creating work for others (multiplying bureaucracy). How about Product Management The pointless Taskmaster. Going through some reddit posts on r/ProductManagement , where people candidly talk about their PM experience, might feel like it is this one of these BS jobs\nOur job is to \u0026ldquo;be like a liquid that fills in the spaces wherever needed\u0026rdquo;\n\u0026ldquo;I would be nothing without my dev teammates, but my devs would probably get on pretty well without me.\u0026rdquo;\n“Stuff would 100% get done without me, and fundamentally my job day to day is a shit umbrella to protect the engineers and designers from nonsense, and to guide the narrative. I\u0026rsquo;m not indispensable.”\n\u0026ldquo;_It is alarmingly easy to get ahead in this field by speaking a solid academic game about product and never shipping anything of substance. This has done real damage to our standing in the tech industry and I don’t see it improving.\nIf it\u0026rsquo;s a Bullshit Job then why so much obsession around it? The idea of a mini CEO, driving strategy, having a say in major decisions, definitely sound appealing. Many Companies and Professionals succumb to this idea.\nHowever, what happens in practice is frightening.\nTop management believes in knowing best what to do and expecting product people to follow their orders. In reality, you start doing bullshit management instead of product management. product professionals become powerless because they are not the ones calling the shots.\nWithout decision power, Product Managers cannot thrive.\nMostly, and I will take a bold guess, 80% of the times, Product Manager is essentially a Backlog Manager or Story Writer. In this scenarios, the only way for a PM to advance in career, is to please stakeholders, rather than focusing on what you should, improving end-users live.\nThats where stakeholder management becomes so important in Product Management in the wrong way. ==When pleasing and obeying stakeholders become the primary aim, a PM will start \u0026ldquo;bullshitting\u0026rdquo; around==.\nThese so called Stakeholder have very less bandwidth to scratch their head with the actual engineering, so they hire PMs to carry the baton on there behalf. ==PM fights the battles with engineering on behalf of the stakeholders. ==\nStakeholders feel important -\u0026gt; purpose solved. Company wants PM PMs feel busy and important -\u0026gt; purpose solved. Professionals want to be PM. Users and the product itself is hampered -\u0026gt; who cares?\n","permalink":"http://localhost:52022/art-of-pm/is-product-management-bs-job/","summary":"\u003cp\u003eYou might already be aware of the widely discussed 2013 article \u003ca href=\"https://strikemag.org/bullshit-jobs/\"target=\"_blank\" rel=\"noopener\"\u003e\n    On the Phenomenon of Bullshit Jobs: A Work Rant by David Graeber\n\u003c/a\u003e\n\u003c/p\u003e\n\u003cp\u003eThe major points (and why they are even more relevant in the age of AI are):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTechnological progress made it ==\u003cem\u003epossible\u003c/em\u003e to reduce working hours== (as Keynes predicted), yet societies have not adopted it. Instead of freeing time, systems manufactured more jobs. \u003ca href=\"https://www.openculture.com/2020/06/when-john-maynard-keynes-predicted-a-15-hour-workweek-in-a-hundred-years-time-1930.html\"target=\"_blank\" rel=\"noopener\"\u003e\n    When John Maynard Keynes Predicted a 15-Hour Workweek “in a Hundred Year’s Time” (1930)\n\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eProductive and manual jobs have ==declined because of automation== (and now because of AI thats declining even faster), but white-collar bureaucratic and pseudo-service roles have exploded.\u003c/li\u003e\n\u003cli\u003eMany workers privately believe ==their work has no real societal value==, creating psychological harm, resentment, and existential dissatisfaction.\u003c/li\u003e\n\u003cli\u003eThe system benefits from people staying busy, disciplined, and dependent. Idle citizens with free time are viewed as \u003cem\u003epolitically dangerous\u003c/em\u003e, so ==employment becomes a moral obligation.==\u003c/li\u003e\n\u003cli\u003eSociety perversely ==rewards low-value or exploitative roles more than essential ones== (nurses, teachers, sanitation workers). This fuels resentment \u003cem\u003eagainst\u003c/em\u003e people with meaningful jobs, not meaningless ones.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHe later published a book on the same topic, and outlined 5 major types of BS jobs\u003c/p\u003e","title":"Is Product Management a BS job?"},{"content":"AI Travel Agent with crewAI and Ollama Traditional OTAs like MakeMyTrip and Booking limit users to fixed filters.\nThis project offers an unrestricted AI search experience based on user intent. An unrestricted AI-powered search experience driven by user intent.\n🔑 Key Features • Personalized Search — Users describe their travel needs naturally.\nExample: “A secluded stay by a riverside within 200 km from Bangalore with Wi-Fi and parking.”\n• AI-Powered Recommendations — Specialized agents interpret intent and fetch the most relevant options.\n• Detailed Itineraries — Automatically generated using dedicated itinerary agents for end-to-end trip planning.\n💡 How It Works Agent Role Description 🧩 Intent Mapper Agent 🧠 Understanding Extracts key details and user preferences from natural language queries. 🔍 Finder Agent 🌐 Discovery Searches for destinations, stays, or activities that best match the extracted intent. 🪄 Formatter Agent ✨ Presentation Structures and refines results into a clean, readable format. 🗺️ Itinerary Agent 🧳 Planning Builds a complete travel itinerary, including suggestions and timelines. ℹ️ Technologies Used\nOllama · CrewAI · Streamlit\nResources\n🔗 GitHub 📝 Read on LinkedIn ","permalink":"http://localhost:52022/projects/ai-travel-agent-for-stay-and-itinerary-planning/","summary":"AI-driven travel planner that provides unrestricted, natural-language-based search and itinerary generation.","title":"AI Travel Agent for Stay and Itinerary Planning"},{"content":"\nPM interview simulator In case you want to skip everything and try the simulator: ☝️\nPractice makes perfect, and nowhere is this truer than in the realm of interviews. The more you practice, read, and learn, the more adept you become. For those preparing for product management interviews at junior and assistant levels, two books often come highly recommended: Cracking the PM Interview and Decode and Conquer. These books are celebrated for their conversational examples between interviewers and candidates, which help readers grasp the interview dynamics effectively. With the advent of powerful tools like ChatGPT, one would expect interview preparation to be easier. Why did I create this? While preparing for PM interviews, we noticed that the responses from general GPTs often lacked the realistic flow and format expected by interviewers. So, we decided to create a solution that bridges this gap, helping you practice effectively and confidently.\nLet\u0026rsquo;s try out the most popular vanilla GPTs with a generic design question Design an alarm clock for the blind\nHere is the response from ChatGPT-4o. And here’s the same question posted to Claude: Let\u0026rsquo;s try one more, this time an RCA question\nOla is experiencing a 25% decrease in daily ride bookings. How would you identify the root cause of this decline and propose solutions?\nLet\u0026rsquo;s see how chatgpt performs And here\u0026rsquo;s how Claude performs Though these are valuable insights, but not exactly how the interviewer expects the answer structure. Let\u0026rsquo;s try to prompt to act as a PM interview candidate and answer the question.\nIt’s clear that the responses are quite generic, lacking the structured and realistic flow expected in an actual interview.\nBridging the Gap: Introducing the ‘Product Management Interview Simulator’ To address this gap, I created a custom GPT tailored specifically to help PM aspirants prepare for interviews. I’ve named it the ‘Product Management Interview Simulator’. Let’s see how the custom GPT handles the same questions: What to expect?\nDetailed and Structured Responses: Get answers that follow the proper format, including clarifying questions and realistic scenarios enacting the interviewer and the interviewee.\nThe Design Question If you have ever prepared for PM interviews, this response looks much more realistic and acceptable right?\nThe RCA Question As you can see, this simulator closely mimics real-world scenarios, asking clarifying questions and following a proper flow and format.\nHow Did I Train the GPT? Creating this custom GPT involved leveraging ChatGPT’s capabilities for training with prompts and custom data. For the training data, I scraped a wealth of mock interviews from various sources:\nPDFs of relevant books\nTranscripts of YouTube videos\nPosts from product management communities\nBlogs dedicated to PM topics\nI then cleaned and structured this data into a JSON format, a snippet as below:\nThe JSON file includes the question, related product, product type, question keyword, question type, and a detailed interaction between the interviewer and interviewee.\nFocus Areas I initially focused on five key question types, which are commonly asked for in PM interviews\nProduct Design\nRoot Cause Analysis (RCA)\nProblem Solving\nProduct Improvement\nProduct Metrics\nBy providing detailed context and employing few-shot prompting techniques, I configured the GPT to understand and format responses appropriately for the approach to different types of questions.\nYour Feedback Matters to Me While the GPT is still a work in progress, I am eager for feedback from the Product Management Community. If you’re interested, please visit the custom GPT, try it out with different types of questions [out of the question-types mentioned above], and share your thoughts.\nHere is the link to the custom GPT:\nPM Interview Simulator Your input will not only help improve the tool but also help to reach fellow PM aspirants in their journey towards acing their interviews. Happy practicing!\nFeedback Your feedback (whether in my inbox, comments, or through a rating) will be immensely valuable.\nYou can click on the down arrow beside the GPT name on top left, and click on \u0026lsquo;Review GPT\u0026rsquo; to post a rating Though you need to be a premium member to rate, however you just need to login to ChatGPT to use the simulator.\nDisclaimer These are LLMs and they can make mistake. They are not a replacement of creative, cognitive. logical and strategic thinking. The responses should only be taken as food for thought while practicing and exploring different types of questions.\n","permalink":"http://localhost:52022/projects/product-manager-interview-simulator/","summary":"Custom GPT model simulating realistic product management interviews.","title":"Interview Simulator"},{"content":"You hear these two words thrown around all the time, and they’re almost always used together. But here’s the thing: they do two completely different jobs. Getting them right is the foundation of all digital security.\nSo what\u0026rsquo;s the deal, exactly?\nAuthentication is the bouncer at the club door checking your ID. Its only job is to answer one question: “Who are you?” Authorization is the VIP wristband you get after you\u0026rsquo;re inside. It tells the bouncer which rooms you’re allowed into. Its job is to answer: “What are you allowed to do?” Authentication always comes first. You can\u0026rsquo;t figure out what someone is allowed to do until you know who they are. That’s it. One proves your identity, the other checks your permissions. Let\u0026rsquo;s break down the most common ways apps do both.\n✅ Authentication: Proving You Are Who You Say You Are Authentication is the first gate you have to pass through. The system needs to trust that you\u0026rsquo;re not an imposter before it lets you in. Here are the methods you see in 99% of modern apps.\n1. Passwords + MFA: The Classic Combo You know the drill. You type in your username and a password you hopefully haven\u0026rsquo;t forgotten. It’s the oldest trick in the book.\nBut we all know passwords alone suck. They get stolen in data breaches, phished by fake login pages, or are just plain weak (Password123!). That’s where Multi-Factor Authentication (MFA) comes in. It’s that second step—a code texted to your phone, a tap on an authenticator app, or your fingerprint.\nThink of it like having two different locks on your front door. Even if a hacker steals your key (password), they still can’t get past the second lock (your phone).\nWhen to use: Literally everything that matters. Banking, email, critical work systems. If you care about the account, it needs MFA.\nThe Good: The Bad: - Everyone gets it. - Passwords are the weak link without MFA. - With MFA, it\u0026rsquo;s incredibly secure against most common attacks. - The extra step of MFA can feel like a hassle, but the security is more than worth it. 2. OAuth \u0026amp; OIDC: The \u0026ldquo;Sign in with Google\u0026rdquo; Button Ever clicked \u0026ldquo;Sign in with Google\u0026rdquo; or \u0026ldquo;Continue with Apple\u0026rdquo; on a new app? That’s OAuth 2.0 and OIDC at work.\nInstead of creating yet another password, you’re telling the new app: \u0026ldquo;Hey, go ask Google if I\u0026rsquo;m legit. They\u0026rsquo;ll vouch for me.\u0026rdquo; OIDC handles proving who you are, while OAuth 2.0 handles the permission part, like letting the app access your name and email (but not your actual password).\nIt’s like getting into a partner\u0026rsquo;s office building because your company ID is trusted there. You don\u0026rsquo;t need a new ID for every building.\nWhen to use: Perfect for any modern web or mobile app that wants to make signup painless. It reduces the friction of making users create another new account.\nThe Good The Bad Super convenient for users. You\u0026rsquo;re putting all your eggs in one basket. If your Google account gets compromised, every app linked to it is at risk. Security is handled by giants like Google or Microsoft who have armies of engineers protecting your account. For developers, it can be a bit tricky to set up correctly. 3. SAML: The Corporate Badge SAML is a fancy technical standard that’s basically built for big businesses. It’s what makes Single Sign-On (SSO) possible in most companies.\nImagine you log into your company\u0026rsquo;s main system (maybe Okta or Azure AD) in the morning. With SAML, you then automatically get access to all your other work apps—Salesforce, Workday, internal dashboards—without logging in again. Your company\u0026rsquo;s identity provider vouches for you.\nIt\u0026rsquo;s like flashing your corporate badge at the entrance and then being able to walk straight into different departmental offices without another check.\nWhen to use: Exclusively for corporate environments. It’s how employees efficiently access tons of business applications with just one login.\nThe Good The Bad Huge boost to employee productivity by eliminating repeated logins. Can be a real beast to set up and manage. IT departments get centralized control over who accesses what. Not really for consumer apps; it\u0026rsquo;s too complex for your everyday website. Enhances overall corporate security with one strong login point. If the main identity provider goes down, everyone loses access to everything. 4. Token-Based Authentication (e.g., JWT): The Concert Ticket Once you log into an app (say, with a username and password), the server often gives you a special \u0026ldquo;ticket\u0026rdquo; – usually a JSON Web Token (JWT). This ticket is like a temporary pass that proves you\u0026rsquo;re logged in.\nEvery time you do something else in the app—click a link, add to cart—your browser automatically shows this ticket to the server. The server quickly checks the ticket to make sure it\u0026rsquo;s valid and who it\u0026rsquo;s for, and boom, you\u0026rsquo;re good to go. The server doesn\u0026rsquo;t need to \u0026ldquo;remember\u0026rdquo; you directly; it just validates your ticket. This is why it\u0026rsquo;s called \u0026ldquo;stateless.\u0026rdquo;\nIt\u0026rsquo;s like getting your hand stamped at a concert. You only show your actual ticket once at the entrance. After that, you just flash your hand stamp to move between different areas.\nWhen to use: Super common in modern web apps (especially single-page apps), mobile apps, and APIs. It\u0026rsquo;s perfect for when you need a scalable, efficient way to keep users logged in across many interactions without heavy server load.\nThe Good The Bad Highly scalable since the server doesn\u0026rsquo;t store session info and can handle many users across many servers. Size matters because tokens carrying too much info can become large and slow things down a little. Flexible because tokens can be used across different services or parts of an application. If stolen, the token is valid until it expires so expiration and revocation need careful management. Secure when signed since tampering is immediately detectable. Stateless challenges make it harder to instantly log out a user everywhere compared to traditional session management. 5. Passwordless Login: The Future is Now This is where passwords finally go to die, making logins both more secure and way simpler.\nMagic Links: You type your email, and the system sends a unique, one-time link to your inbox. Click it, and you’re logged in—no password needed. Passkeys: This is the real game-changer. Instead of a password, your device (phone, laptop) creates a unique, super-secure cryptographic key. You use your device\u0026rsquo;s built-in biometrics (like face ID or fingerprint) to unlock this key, which then securely logs you into the website. These keys are virtually un-phishable because they never leave your device and are unique for each site. It\u0026rsquo;s like ditching your house keys entirely and just using your fingerprint to open your front door and automatically disarm the alarm.\nWhen to use: Increasingly adopted by apps that want to offer top-tier security and a super smooth login experience, moving away from vulnerable passwords. Passkeys are quickly becoming the industry standard.\nThe Good The Bad Eliminates password risks since there is no password to steal which removes phishing, brute force attacks, and password breach exposure. User adoption can be slow because many people are unfamiliar with these methods. Blazing fast and simple. Magic links are one click and passkeys use familiar biometrics for very fast logins. Email vulnerability for magic links. If your email account gets hacked, the links are exposed. Highly secure, especially passkeys which resist almost all common hacking methods. Device dependency for passkeys. Losing or breaking the primary device requires a solid recovery plan. 🔐 Authorization: What Are You Allowed to Do? Okay, so you\u0026rsquo;ve proven who you are. The system trusts you. Now what? This is where authorization steps in. It\u0026rsquo;s the system checking your \u0026ldquo;VIP wristband\u0026rdquo; to see which areas you can access and what actions you\u0026rsquo;re cleared to perform.\n1. RBAC: The Workhorse (or, The T-Shirt Size Model) Role-Based Access Control (RBAC) is the most common authorization method out there. It\u0026rsquo;s simple and effective.\nInstead of giving every single user individual permissions (imagine managing that for a company of 1,000!), you group users into \u0026ldquo;roles.\u0026rdquo; Think Admin, Editor, Viewer, Manager, Employee. Each role gets a specific set of permissions. You assign a user to a role, and boom, they automatically get all those permissions.\nIt\u0026rsquo;s like in a company: you get access to certain files and systems based on your job title (your role), not by individually requesting permission for every single document.\nWhen to use: Great for almost any application where users can be logically grouped. Perfect for business applications with clear hierarchies or teams with distinct responsibilities.\nThe Good The Bad Easy to understand and intuitive for users and administrators. Can be rigid. If roles are not well defined, users may end up with too much or too little access which leads to permission creep or frustration. Simple to manage. Assigning roles is faster and less error prone than assigning individual permissions. Limited granularity. It struggles with extremely specific or temporary access needs. Efficient because centralized permission management at the role level saves significant time. 2. OAuth 2.0 Scopes: Asking for Permission Remember OAuth 2.0 from the authentication section? While it helps with logging in, its true superpower is authorization, especially for APIs (Application Programming Interfaces).\nWhen a third-party app wants to connect to one of your services (like your fitness tracker linking to Google Fit), it doesn\u0026rsquo;t get full access to everything. Instead, it asks for specific \u0026ldquo;scopes\u0026rdquo;—think of them as clearly defined permissions. For example, your fitness tracker might ask for read:activity_data (to see your steps) and write:activity_data (to record new workouts), but not delete:all_my_data. You, the user, then get to review and approve (or deny) these specific requests.\nIt’s like an app saying, \u0026ldquo;Hey, can I just look at your photos, not edit or delete them?\u0026rdquo; You give it that specific permission, and nothing more.\nWhen to use: Essential for authorizing third-party applications to access specific resources or perform actions on your behalf within an API. Super common when different online services need to talk to each other.\nThe Good The Bad Granular control allows you to specify exactly what an app can access which boosts privacy and security. Complexity in designing and managing different scopes for developers. User empowerment by giving limited permissions rather than all or nothing access. Over privileging risk if scopes are not designed carefully which can create security loopholes. Standardized and widely accepted as a secure way for services to interact. 3. ABAC: The Hyper-Specific Rule Book Attribute-Based Access Control (ABAC) is the advanced, highly dynamic authorization model. It throws out simple roles and instead makes access decisions based on a bunch of \u0026ldquo;attributes\u0026rdquo; (characteristics) from multiple sources.\nThese attributes can be about:\nThe User: Their department, job title, security clearance. The Resource: The document\u0026rsquo;s sensitivity, type, or data classification. The Environment: The time of day, geographical location, the device\u0026rsquo;s security status. The Action: Whether the user is trying to read, write, or delete. A powerful \u0026ldquo;policy engine\u0026rdquo; then evaluates these attributes against a set of rules. For example: \u0026ldquo;Only users from the \u0026lsquo;Finance\u0026rsquo; department with a \u0026lsquo;Manager\u0026rsquo; role can approve expenses over $1,000 between 9 AM and 5 PM on a company-issued, encrypted device, if they are physically in the \u0026lsquo;New York\u0026rsquo; office.\u0026rdquo;\nThis isn\u0026rsquo;t magic; it\u0026rsquo;s just very sophisticated logic.\nWhen to use: Ideal for incredibly complex environments with super dynamic access needs. Think large enterprises, cloud platforms, or systems with strict compliance requirements where decisions need to be made based on many real-time factors.\nThe Good The Bad Unparalleled flexibility that supports highly detailed and dynamic access rules which adapt to changing conditions. Seriously complex to implement and manage. It requires strong systems for policy definition and enforcement. Extreme granularity that allows access decisions based on a very wide range of specific criteria. Hard to debug because large rule sets and many attributes make it tough to understand why access was granted or denied. Reduces role sprawl since policies rely on attributes instead of creating endless roles. Performance hit because real time evaluation of many attributes can introduce slight delays. ","permalink":"http://localhost:52022/tech-for-pm/authentication-vs-authorization-whats-the-difference/","summary":"\u003cp\u003eYou hear these two words thrown around all the time, and they’re almost always used together. But here’s the thing: they do two completely different jobs. Getting them right is the foundation of all digital security.\u003c/p\u003e\n\u003cp\u003eSo what\u0026rsquo;s the deal, exactly?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAuthentication is the bouncer at the club door checking your ID.\u003c/strong\u003e  Its only job is to answer one question:  \u003cstrong\u003e“Who are you?”\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAuthorization is the VIP wristband you get after you\u0026rsquo;re inside.\u003c/strong\u003e  It tells the bouncer which rooms you’re allowed into. Its job is to answer:  \u003cstrong\u003e“What are you allowed to do?”\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eAuthentication  \u003cem\u003ealways\u003c/em\u003e  comes first. You can\u0026rsquo;t figure out what someone is allowed to do until you know who they are.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThat’s it. One proves your identity, the other checks your permissions. Let\u0026rsquo;s break down the most common ways apps do both.\u003c/p\u003e","title":"Authentication vs. Authorization - What’s the Difference?"},{"content":"Some Intro Think of it this way. You started working and earning in your early twenties. You were basking in the glory of the newfound financial freedom. You wanted more, and hence you toiled, you learned the industry, learned about business and consumers, and slowly you moved up the ladder, earning more and more. But there was something that kept building up - Taxes and EMIs, responsibilities, deteriorating health and relationships, creeping up depressions and anxieties. You didn’t pay much attention to it earlier and didn’t invest properly (in health, wealth and relationships). Now they have started hurting you. Though the ==influx of wealth has grown, you are not able to enjoy it like you used to do. You want to go out and party, roam the world, and live carefree like you used to do, but things keeps pulling you back.==\nThis is similar to what Tech Debt is. You started building a product, and applied quick fixes and hacks to grow it without thinking about overall architecture and optimizations. You went on adding more and more features to the customers\u0026rsquo; (and the sales teams\u0026rsquo;) likings, kept on experimenting while ignoring the health of the system. Pretty soon the health deteriorates and it’s time to pay the accumulated debt. And there you are, tussling between how much time should you need to allocate to new feature requests and how much to improve the system\u0026rsquo;s health.\nPS: You can refer back to this analogy throughout this article, and also use it whenever you need to explain tech debt to the VPs. You can thank me later 😉\nWhy is making that small tweak taking weeks? A picture tells a thousand words, and this picture tells an entire story, a horror one though. This is exactly what accumulating tech debt can do to your system. As a PM, you aspire to do great things for your customers, no matter if you are at a startup, finding the product-market-fit, or at a Fortune 500 company, delivering more and more value based on the market-fit.\nIn both cases, ==the faster you go, the more tech debt you are accumulating,== they are proportional. Pretty soon you are bound to feel like the tech debt is weighing down your company from achieving the vision you have for the org. How many times we have gotten caught up in situations like\nMore and more bugs and hence service tickets opening up, taking up more and more bandwidth of support and tech team Fixing that small bugs, or making that small tweak is taking weeks. The velocity of the engineering team going down, and business teams complaining about engineering not delivering These are indications that you have accumulated quite a good amount of tech-debt.\nCan you escape tech debt? It is very common for an early-stage product to accumulate tech debts. You need to move fast to get out as much customer-facing functionality as you can and there’s no time to pay attention to stability or scalability. At this stage, it is necessary to a large extent.\nA little debt speeds up delivery quite a lot.\n==It is OK to make some tradeoffs and not build things in the most robust way as long as you get things out quickly.== You can, rather, you have to keep on saying, ‘Not Today’ for a while.\nBut once the company really starts growing and you start bringing in hundreds of customers, that doesn\u0026rsquo;t work anymore; because the more customers you have, the more your engineering team is going to have to address escalations that happen. The more Functionality you have put out there, the more will be the demand for maintaining and improving that functionality because, like a very wise man once said:\n“No feature is ever done after version one”.\nAnd of course, the more customers you\u0026rsquo;re supporting on your platform, the more those shortcuts that the team did on the technical side would surface. So the team is going to be very distracted with fixing things. And finally, what you are left with is a very complex and very high maintenance product that keeps on breaking and leaking from here and there.\nSo the question is , how much tech debt can you afford to accumulate? (Callback: ==similar to how much can you let that pot-belly or the back-ache to grow, or that relationship to deteriorate before you start taking care of them.)==\nWhen chaos arises, the PM will be hunted down No matter how much we cover up, ==every company at the end of the day is a sales-driven company.== When you were building and delivering fast, the sales and business team got the hang of it. Now when things start breaking and delivery speed reduces, there will be an impact on growth. When this happens, the scapegoat will always be the Product Team.\nThere’s not much use in making the business understand the tech debt fallacy either. They are not interested in hearing stories, and in one way it is just to some extent. Their incentives (and somehow their life) depends on sales. When sales are hampered because of product issues, there\u0026rsquo;s an easy punching bag.\nYou can try reciting a Shakespearean monologue in that meeting room, being all philosophical describing how you were the greatest person on the earth when you delivered faster than light and now it’s time to repay. ==As a PM, however, you have to end up taking the blame anyway. ==, So it\u0026rsquo;s better to be prepared beforehand.\nJTBD: A PM needs to be very stern here and inform the Stakeholders that the team will be focussing more on putting the house in place and for a while, only very high-impact features will be addressed. You need to show them the picture of the doom and rightly scare them to the core that if things are not taken care of right now, the entire economy of the organization will break. How operations costs will go up, SLAs won\u0026rsquo;t be met, the speed of delivering value will get lesser and lesser, and ultimately lesser conversions and increasing refunds.\nStrategies to retire that tech-debt If you walk around the internet, you will find 2 ways people prefer to solve tech debt, having a certain fixed bandwidth allocated in every sprint and/or having a dedicated team.\nHaving delivered three 0-to-1 products and two 1-to-N products, one thing I have understood is that One-size doesn’t fit all, and the strategy will vary based on the situation. If we can just be aware of 3 things, we can have a robust approach to solving the tech debt fallacy.\n1. Gauging the Runway It\u0026rsquo;s absolutely OK to let tech debt accumulate but you need to understand the length of the runway on which you are going to operate your flight. If there is a long runway and you are preparing for that long flight, you can afford to build scrappily for a longer time. But as soon as the flight prepares for take-off, mistakes can cost a humungous amount.\nJust like, you don’t need to start investing from the first salary itself, you can take time to enjoy. but you need to get into action as soon as the credit card bills pile up.\nMoving on from philosophies and analogies, in reality, the JTBD is to keep a sharp eye on a few things, the growth curves, the number of bugs and issues, the tech teams\u0026rsquo; velocity, and their allocation in resolving bugs vs building features. It’s a combination of all these things that will paint a clear picture of how soon you need to start taking Tech-Debt seriously.\n2. Know the breaking points thoroughly. In today\u0026rsquo;s world, technology is changing faster than in the blink of an eye. It is always tempting to scrap the existing one and adopt the new one as it is more flashy, but at what cost? Will it solve the current breakages, Do you have a log of which scrappy codes and architectures were delivered to keep up with the pace of delivery\nBefore you fall into the trap of changing to new, do remember, the US defense got rid of Floppy disks used to control US nuclear weapons in 2019. For half a century they used those 8-inch disks, simply because the floppy disks did the job well and securely.\n(Opinion)\nA feasible way to approach this is to Empower the Engineering team to maintain a Confluence page on “Tech Radar”__. Whenever a feature is delivered, let the developer put in notes over here on how might this feature behave at a scale, what has the potential to break, and if there is something we should look out for in the near future. Encourage the tech team to keep revisiting this doc during every sprint retro. The Tech Radar might also include frameworks, and architectures that “want to adopt”, “want to assess”, and ”want to avoid”.\nThis single set of artifacts can be the guiding spirit when you get drowned in that sea of tech debt.\n3. Empower the engineering team and provide the leeway.\nAt the end of the day, the tech team will resolve the tech debt, for themself, for the business, and for the customer, not the PM. So, whats your role here?\nAs a PM, your role is to judge the impending doom and empower the Tech team.\nAs soon as you have figured out that the runaway is ending, and things have started showing the potential to fall apart, you have to take some steps.\n==Reduce the size of the feature backlog through ruthless prioritization ==. If you as a PM budge into business and customers to deliver more and more features, it will hurt back. Don’t take up fancy features in your roadmap if you feel the existing core frame is going to break soon. Ask harsh questions about the impact of a new feature. If the house is breaking then decorations can wait. ==Take the Engineering leaders in confidence== and make them understand why you are bringing lesser feature requests to the table. Empower them to dedicate time to retiring tech-debts slowly and steadily. ==Leave the strategy of allocation to Engineering leaders.== Don’t fuss about the percentage time allocation or the number of engineers working on them. Instead, work with them to create a system of tracking and monitoring metrics indicating system health. ==Keep track of the metrics regularly.== The next time when you are charged about features not getting delivered, these metrics are what will come in handy. The most critical task of a PM in regard to tech debt is to foresee the impending doom. You can wait and enjoy the summer but winter will eventually come for all, and you have to be prepared.\nBefore winter strikes, just as a cold wind starts blowing, it’s high time to reduce the load of new customer-facing features on the engineering team. Build those logs, those observability services and dashboards, optimize those APIs and queries, build the internal tools, invest time on regression and unit testing, perma-fix those quick-fixes, frameworks, and architecture.\nSooner or later, you have to pay the debt, it’s up to you to decide when and how?\n","permalink":"http://localhost:52022/art-of-pm/a-pm-always-pays-his-tech-debt/","summary":"\u003ch2 id=\"some-intro\"\u003eSome Intro\u003c/h2\u003e\n\u003cp\u003eThink of it this way. You started working and earning in your early twenties. You were basking in the glory of the newfound financial freedom. You wanted more, and hence you toiled, you learned the industry, learned about business and consumers, and slowly you moved up the ladder, earning more and more. But there was something that kept building up - Taxes and EMIs, responsibilities, deteriorating health and relationships, creeping up depressions and anxieties. You didn’t pay much attention to it earlier and didn’t invest properly (in health, wealth and relationships). Now they have started hurting you. Though the ==influx of wealth has grown, you are not able to enjoy it like you used to do. You want to go out and party, roam the world, and live carefree like you used to do, but things keeps pulling you back.==\u003c/p\u003e","title":"A PM always pays his (Tech) Debt"},{"content":"AI Agent to analyze your snaps. How about a crew of AI agents who can sift through your snaps, pick the absolute best ones, and even whip up the perfect Instagram captions? Say goodbye to the endless selection and caption conundrums. With AI agents, we are stepping into the future of AI-powered creativity, where your snaps are transformed into Instagram hits without breaking a sweat.\nThe \u0026ldquo;Insta Influencer\u0026rdquo; Image Analyzer This nifty app is like having your very own Instagram manager on speed dial. Simply upload a bunch of images, and let the AI do the heavy lifting. It doesn’t just pick the best photo; it goes a step further by crafting a caption that’s not only on-trend but tailored to the vibe of your image. How? By harnessing the power of advanced AI models that understands images and what makes a post pop on Instagram.\n🔗 Check out the Git repo here: https://github.com/dibyendutapadar/ai-agent-image-analyzer Demo: I provided the app some pics from Goa trip to analyze from. The response was quite astonishing form 8b models run on local. Notice how detailed the image descriptions are.\nThe Tech That Makes It All Happen Large Language Models (LLMs) At the heart of this magic is obviously LLMs (Large Language Models). LLMs have been game-changers in making our interactions with technology more natural, almost like chatting with a super-smart friend.\nLLaMA and LLAVA Now, let’s talk about two key players: LLaMA (Large Language Model Accelerated) and LLAVA (Large Language Vision Agent). LLaMA is one of your go-to LLM for processing text, while LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding.\nTogether, they’re the dynamic duo that enables this app to seamlessly juggle both text and images.\nI have used 8b models of both, but if you have access to superior computing power (read GPU) you can use larger models as well\nAI Agents and Crew AI Here’s where it gets even cooler—AI agents. These are autonomous helpers that handle tasks for you. In our app, we use CrewAI to coordinate these agents, making sure everything runs smoothly. It’s like having a team of experts working behind the scenes to ensure your photos and captions are on point.\nHow It All Comes Together So, how exactly we select from photos into Instagram-worthy posts? Let’s break it down:\nUpload Your Photos: Start by uploading your images—whether they’re in PNG, JPG, or JPEG format. AI Image Analysis: The magic begins as LLAVA gets to work, analyzing your images to extract detailed descriptions. It’s like having an art critic and a storyteller rolled into one, interpreting each photo. Crafting the Perfect Caption: Armed with these descriptions the agent then picks the best image and writes a caption that’s primed for maximum engagement. It considers everything from trending hashtags to current events. AI Agent in Action: Crew AI coordinates the process, ensuring that the agent’s decisions are spot on. The agent doesn’t just pick a photo and write a caption; it does so with a clear strategy in mind, maximizing your post’s potential to go viral. Your Ready-Made Instagram Post: Finally, the app presents the results in a clean, easy-to-use format. You get a detailed breakdown of all your photos, the chosen winner with an explanation, and a caption ready to be copied and pasted straight into Instagram. And the best part about this,\nAll this is achieved without any need to train the model on own data.\nGone are the days of scrambling to gather massive datasets just to get started. These pre-trained models give you a serious head start. Sure, you can fine-tune them with your specific data to make them even better, but as you can see from the results, they’re already delivering impressive outcomes right out of the box.\nWhat\u0026rsquo;s the big deal? ChatGPT can already do this. Of course, it can—but there’s a catch. While GPT-4 and its image processing capabilities can handle similar tasks, these features often come with a price tag.\nPlus, our app is a playground for these ideas, offering a fun, user-friendly way to explore what AI can do with your images. Now, imagine scaling this up for industrial use cases. Organizations can employ similar methodologies to tackle specific challenges unique to their operations—without the worry of sharing sensitive information with a third-party service like ChatGPT.\nIn-house AI solutions provide a tailored approach, ensuring privacy and control over proprietary data.\nBeyond a toy app to Organizational use cases Below are some industry use cases where images are too sensitive and confidential to be exposed to third part service provider. Getting LLMs deployed on premise, can help maintian the secrecy and getting the job done.\nHealthTech Medical Imaging: Assist in diagnosing conditions from X-rays to MRIs. Lets see this in action. It was fascinating to see how much can be achieved without any fine tuning from a 8b vision model and a 8b text model with 30 lines of code on a personal laptop\nEdTech Grading Exams: Automate grading for handwritten exams, ensuring fairness and speed. (I have a hunch that with the democratisation of GenAI, handwritten exams are going to come back. For EdTech enthusiasts, you can read this study on the effect of GenAI on education) Let\u0026rsquo;s see how an untuned model can provide a head start for evaluation of answer scripts.\nIt can also be used in other sectors where information is absolutely confidential\nFinTech Customer Verification: Automate document verification with image analysis. Insurance Claims Processing: Quickly assess damages with image evaluation. Technologies Used: LLaMA, LLaVA, CrewAI, Streamlit\n🔗 GitHub 📝 Read on LinkedIn ","permalink":"http://localhost:52022/projects/image-analysis-with-llava-and-llama/","summary":"Uses multimodal LLMs for domain-specific image understanding in marketing, education, and healthcare.","title":"Image Analysis with LLaVA and LLaMA"},{"content":"🌟 𝗖𝗮𝗻 𝘄𝗲 𝗰𝗿𝗲𝗮𝘁𝗲 𝗮𝗻 𝗮𝗱𝗮𝗽𝘁𝗶𝘃𝗲 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴 𝗔𝗽𝗽 𝘂𝘀𝗶𝗻𝗴 𝗟𝗟𝗠 𝘄𝗶𝘁𝗵𝗼𝘂𝘁 𝗮 𝗾𝘂𝗲𝘀𝘁𝗶𝗼𝗻 𝗮𝗻𝘀𝘄𝗲𝗿 𝗱𝗮𝘁𝗮𝘀𝗲𝘁? 🤔 I gave it a try! 🚀\nI built an app on Streamlit utilizing LlamaIndex . It takes in key information fromt he user , wraps it in a prompt, and uses Groq inference API for a small 𝗟𝗟𝗠 (𝗹𝗹𝗮𝗺𝗮𝟯.𝟭-𝟴𝗯-𝗶𝗻𝘀𝘁𝗮𝗻𝘁) to generate responses. By leveraging 𝗽𝘆𝗱𝗮𝗻𝘁𝗶𝗰, we generate structured output in format that can be made as a multiple choice question, and the in-between data storage plus proficiency calculations are handled by 𝗦𝗤𝗟𝗶𝘁𝗲. Based on the calculation, after each answer, the LLM is prompted with the difficulty level of the next question to be generated.\n🔗 You can try out the app here: https://lnkd.in/gtXUHWMp 💡 𝗙𝗶𝗻𝗱𝗶𝗻𝗴𝘀: The answer is both yes and no! We can jumpstart by using the data the LLM was trained on. Even a small model does a fair job, but limited and un-curated knowledge base hampers the experience.\nHowever, this can be tackled with 𝗥𝗔𝗚 𝘁𝗲𝗰𝗵𝗻𝗶𝗾𝘂𝗲𝘀. Where the exact contextual data can be retrieved from documents (books, PDFs, databases, PPTs, Videos) and then augmented by using an LLM. Particularly Graph RAG is showing promising potential for these kind of knowledge driven problem\n🤔 𝗣𝗼𝘀𝘀𝗶𝗯𝗶𝗹𝗶𝘁𝗶𝗲𝘀: 🎯 𝗔𝗱𝗮𝗽𝘁𝗶𝘃𝗲 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴: Generates questions matching a learner\u0026rsquo;s proficiency. Using knowledge graph the system can even be made to understand pre-grade learning gaps of the learner.\n📚 𝗗𝘆𝗻𝗮𝗺𝗶𝗰 𝗖𝗼𝗻𝘁𝗲𝗻𝘁: Creates content tailored to individual learning gaps (text, image, audio, video via multimodal LLMs). Should be an improvement over the long MOOCs, which starts and ends at the same level and follows the same path for every learner.\n🧠 𝗖𝗼𝗻𝘁𝗲𝘅𝘁𝘂𝗮𝗹 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴: Customizes content to the learner\u0026rsquo;s interests, like teaching math to a cricket enthusiast student using cricket examples. And the best part, we don\u0026rsquo;t need to have curated datasets to do that, it can be generated. 🏏⚾\nAI\u0026rsquo;s (Specifically GenAI\u0026rsquo;s) impact on education is being debated heavily, and rightly so. But can we ignore its potential? I don’t think we can! 🤖✨\nTechnologies Used: Python, Groq, LlamaIndex, Llama 3.1-8B, SQLite, Streamlit, Pydantic\n🔗 GitHub 📝 Read on LinkedIn 📱 App Link ","permalink":"http://localhost:52022/projects/adaptive-learning-without-question-answer-dataset/","summary":"Generates adaptive learning assessments dynamically using GenAI and proficiency tracking.","title":"Adaptive Learning without Question-Answer Dataset"},{"content":"An AI-powered digital platform designed for edTech organizations, enabling the digital capture of pen-and-paper test transcripts and scores. This facilitates the enhancement of student learning outcomes by allowing for the personalization of educational offerings.\nMy Role - 2021, Product Manager, Byjus -\nI worked with our multidisciplinary team to take AssessEd from implementation to growth, maturity and retirement (replacing with an enhaced platform). I conducted research, user interviews, managed onboarding, oversaw AI training, and worked closely with the CEO, product designer and engineering managers to enhance, mature and integrate the system with multiple existing systems.\nIntroduction (Origin Story) The COVID-19 pandemic brought about a seismic shift in the education sector, propelling online learning to the forefront. While this transition offered several advantages, it also posed challenges, particularly concerning traditional pen and paper tests. Education boards were hesitant to abandon the tried-and-tested offline mode of assessments due to its numerous benefits. In response to this dilemma, a groundbreaking test-taking platform was developed, seamlessly merging the advantages of both online and offline assessment methods.\nThe Challenge: Problem Statement During the COVID-19 pandemic, the online education sector experienced an unprecedented boom, leading to a significant decrease in offline interactions. However, pen and paper tests continued to hold importance due to the following four notable benefits:\nAuthenticity and Security: Pen and paper tests provide a tangible experience, ensuring the authenticity of the examination process. With controlled environments and invigilation, the integrity of the assessments remains intact.\nPhysical Interaction: Offline tests foster face-to-face interactions between teachers and students, facilitating a more personal and engaging educational experience. Non-verbal cues and direct communication enhance the learning process.\nCognitive Retention: Research suggests that taking notes by hand improves information retention and understanding. The act of writing on paper helps students synthesize knowledge, boosting memory and comprehension.\nEquality and Accessibility: Pen and paper tests ensure equal access for all students, regardless of their socioeconomic background or internet availability. They level the playing field, minimizing the disadvantages faced by students lacking reliable internet connections.\nThe Solution To address the challenges faced in conducting pen and paper tests during the online education boom, a comprehensive test-taking platform was developed. This innovative solution enabled teachers to conduct assessments and allowed students to attempt tests using both online and offline modes, combining the best of both worlds.\nThe Product features: The test-taking platform introduced a dual interface that catered to the needs of both teachers and students. Here\u0026rsquo;s an overview of the features and benefits it offered:\nAuto-creation of Assessments: Teachers can easily create assessments within the platform, with smart-assist by the system specific to the students.\nSeamless Test Distribution: Teachers could distribute assessments digitally or in physical format, depending on the mode of interaction. Online distribution made it convenient for students to access assessments remotely, while physical copies catered to offline classroom scenarios.\nHybrid Answer Sheet Upload: Students had the freedom to answer questions on physical answer sheets or within the platform\u0026rsquo;s digital interface. For Homeworks, students scanned their answer sheets through app and uploaded them, while teachers collected and scanned physical answer sheets in an offline test setting.\nAI-Driven Image Server: The system employed an AI-driven image server to read, segment, and evaluate the answers within minutes. This automation streamlined the evaluation process, making it swift, efficient, and effortless.\nInsights for Personalized Learning: By digitizing answer scripts and scores, the platform gathered valuable insights. These insights were fed into a student data-lake, enabling the creation of personalized learning models and assessments. Students could benefit from tailored educational content, catering to their unique needs, and enhancing their learning outcomes.\nMetrics 10k+ personalized offline assessments are conducted each week for 200k+students. More practice leads to better outcome\nThe collective outcome of the students average scores improved by 7 percentage points over an academic year\nThe time taken to conduct, evaluate and publish the evaluated papers were made to 2.5 days by ingesting technological benefits in the offline test taking medium\n","permalink":"http://localhost:52022/blog/byjus-assessed/","summary":"\u003cp\u003e\u003cem\u003eAn AI-powered digital platform designed for edTech organizations, enabling the digital capture of pen-and-paper test transcripts and scores. This facilitates the enhancement of student learning outcomes by allowing for the personalization of educational offerings.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ree\" loading=\"lazy\" src=\"https://static.wixstatic.com/media/f3d706_a2c9e1602581406a8b5f4beebc0ee438~mv2.png/v1/fill/w_1199,h_749,al_c,q_90,enc_avif,quality_auto/f3d706_a2c9e1602581406a8b5f4beebc0ee438~mv2.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"my-role\"\u003e\u003cstrong\u003eMy Role\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003e- 2021, Product Manager, Byjus -\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eI worked with our multidisciplinary team to take AssessEd from implementation to growth, maturity and retirement (replacing with an enhaced platform). I conducted research, user interviews, managed onboarding, oversaw AI training, and worked closely with the CEO, product designer and engineering managers to enhance, mature and integrate the system with multiple existing systems.\u003c/p\u003e","title":"AssessEd - Bridging gap between the digital EdTech world and Offline Assessments."},{"content":"Origin Story In 2016, despite holding over 50% of the market share, the unauthorized car repair workshops relied on manual pen and paper processes. They failed to follow the digital trend, missing out on benefits that the taxis and food delivery industry was gaining. As a consequence, they were unable to provide their customers with an improved and transparent experience, and they also failed to optimize their daily operations by harnessing the power of technology.\nMy Role As the sole product manager, I led the complete product lifecycle, beginning with ideation, conducting user research, facilitating design sprints, conducting initial user interaction tests with designs, developing the minimum viable product (MVP), and ultimately overseeing the full-fledged rollout, iterating throughout multiple sprints.\nThe Product DearO (Digitally enhanced automotive repair operations system) was ideated to transform the automobile service industry, making day-to-day workshop operations simpler, faster, and smarter than ever before. With its comprehensive suite of features, DearO streamlines workflow, enhances customer communication, and optimizes inventory management, revolutionizing the way a small-medium automotive service businesses operated.\nAdvantages and Features: Mobile-Friendly and Cost-Effective: DearO is a mobile-friendly solution that requires no installation or setup costs, allowing businesses to quickly adopt and integrate the system into their operations.\nCloud-Based Security: With a cloud-based infrastructure, DearO ensures high-level security for workshop data, protecting sensitive information and providing peace of mind.\nPreemptive Damage and Belongings Management: DearO enables workshops to note and share scratches, damages, and personal belongings in vehicles with customers in advance, minimizing disputes and enhancing transparency.\nGST-Enabled Invoicing: DearO features a smart, GST-ready invoicing system that simplifies and accelerates the billing process, saving time and ensuring compliance.\nSeamless Communication: Share invoices and job cards with customers via WhatsApp or email, providing convenience and flexibility to suit their preferences.\nSwift Digital Job Card Creation: Create digital job cards in a matter of seconds, reducing paperwork and improving efficiency.\nCentralized Dashboard: Monitor all job cards and invoices from a user-friendly dashboard, ensuring streamlined operations and comprehensive oversight.\nReal-Time Customer Updates: DearO enhances customer satisfaction by providing automatic real-time status updates through a personalized web portal, keeping customers informed and engaged.\nSmart Labor and Parts Pricing: Benefit from smart search functionality that suggests labor and parts pricing based on the make and model of cars and bikes, simplifying cost estimation.\nAuto-Suggestion and Service Schedules: DearO offers auto-suggestions for jobs based on vehicle history and inspection, as well as parts item recommendations aligned with manufacturers\u0026rsquo; recommended service schedules.\nIMPACT Adopted by 1k paid users within a year of launch. The users found the following benefits from adopting the product\nIncreased Business Revenue: Enhance revenue streams with features such as vehicle inspection reports and built-in service schedules that facilitate upselling and efficient service planning.\nImproved Profitability: Avoid time and costs associated with handling customer disputes related to fuel theft, item theft, unwanted jobs, damages, and scratches allegations.\nElevated Customer Experience: Provide automated real-time service updates, vehicle inventory reports, job card details, and invoice statuses, elevating customer satisfaction and loyalty.\nEnhanced Business Monitoring: Leverage end-of-day reports on job cards and invoicing to gain insights and optimize operations. Manage inventory effortlessly through a simple purchase system and auto-updates on parts invoicing.\nApp Link ","permalink":"http://localhost:52022/blog/dearo-wms/","summary":"\u003ch2 id=\"origin-story\"\u003eOrigin Story\u003c/h2\u003e\n\u003cp\u003eIn 2016, despite holding over 50% of the market share, the unauthorized car repair workshops relied on manual pen and paper processes. They failed to follow the digital trend, missing out on benefits that the taxis and food delivery industry was gaining. As a consequence, they were unable to provide their customers with an improved and transparent experience, and they also failed to optimize their daily operations by harnessing the power of technology.\u003c/p\u003e","title":"DearO - the simplest and smartest Automobile Workshop Management system"},{"content":"\nThe Product Khan for Educators is a powerful tool built on top of Khan Academy\u0026rsquo;s world-renowned educational content, designed specifically for teachers. Khan for Educators revolutionizes the teaching and learning experience by providing a comprehensive set of tools for tutors to create classes, add students, assign videos and assessments, and track individual progress and mastery levels at a concept level.\nCore Features Class Creation: Teachers can effortlessly create classes, organizing students and content in a streamlined manner.\nIndividual Student Logins: Each student receives a personalized login, allowing them to access assigned materials and track their progress.\nAssignments and Assessments: From the teacher\u0026rsquo;s tools section, educators can easily assign videos and assessments to students, tailoring the learning experience to meet their unique needs.\nProgress Tracking: Khan for Educators provides detailed insights into student scores, engagement, performance, and mastery on individual concepts. This information enables teachers to identify areas of improvement and deliver personalized interventions.\nMy Role In my capacity as a Product Manager, I was responsible for optimizing and productising the base Khan for Educators model to align with the specific requirements of the Indian education ecosystem. This involved developing supplementary products and features to facilitate the adoption of the solution by state government education boards, enabling them to seamlessly implement it in public schools across the state.\nIn lieu of Khan Academy\u0026rsquo;s vision, \u0026ldquo;to provide a free world-class education for anyone, anywhere\u0026rdquo;, we made the strategic decision to concentrate on public schools, an untapped market with vast potential. Meanwhile, the private and paid education sector was becoming highly competitive, crowded with numerous EdTech organizations, resulting in a saturated market, often referred to as a red ocean.\nHowever, introducing our offerings to public schools through state education departments proved to be a challenging endeavor. Nevertheless, we persevered and successfully maneuvered through those obstacles along the way.\nChallenges Solutions Most public school teachers lacked the technical proficiency to self-adopt a tech driven platform 1. Built Teacher training modules, on self completion automatic certifications will be provided\n2. Built Whatsapp based support chatbot to help teachers at each step of operation and to communicate with teachers to instill confidence Creating profiles of students, teachers across thousands of schools was a challenge. The existing system supported creation of profiles one-by-one at an individual level, and this method would have called for too much operational overhead. Developed a complete rostering system that incorporates UDISE and state education board data, freeing teachers of all operational work and to concentrate on what they do best- teaching. There was a lack of momentum to adopt a new system from the start. Introduced and executed LearnStorm , a campaign where completion of learning tasks is gamified, and prizes are awarded to the class with most points. over 40,000 teachers across India participated with their classes of students The Government state education board required an overview of engagement and outcome of students at multiple levels Took advantage of GCP\u0026rsquo;s integration with Google Data Studio and built actionable dashboards for the state govt education department that enabled them to track engagement and impact on real time basis. Impact Growth The entire package of solution was adopted by multiple state governments, owing to which the Monthly Active Users of Khan Academy in India grew from 50,000 to 5 lakh in one year.\nLearning Outcome Punjab - the highest adopter of the solution achieved Rank 1 amongst all states in the National Achievement Survey 2021 ","permalink":"http://localhost:52022/blog/khan-for-tutors/","summary":"\u003cp\u003e\u003cimg alt=\"ree\" loading=\"lazy\" src=\"https://static.wixstatic.com/media/f3d706_352ca2ef376a446f9a84a7d197387b11~mv2.png/v1/fill/w_1216,h_554,al_c,q_90,enc_avif,quality_auto/f3d706_352ca2ef376a446f9a84a7d197387b11~mv2.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"the-product\"\u003eThe Product\u003c/h2\u003e\n\u003cp\u003eKhan for Educators is a powerful tool built on top of Khan Academy\u0026rsquo;s world-renowned educational content, designed specifically for teachers. Khan for Educators revolutionizes the teaching and learning experience by providing a comprehensive set of tools for tutors to create classes, add students, assign videos and assessments, and track individual progress and mastery levels at a concept level.\u003c/p\u003e\n\u003ch2 id=\"core-features\"\u003eCore Features\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eClass Creation: Teachers can effortlessly create classes, organizing students and content in a streamlined manner.\u003c/p\u003e","title":"Khan Academy for Tutors"},{"content":"In the ever-evolving landscape of education, the integration of technology has become paramount. My journey as a product manager at Byju\u0026rsquo;s, a leading ed-tech company, reflects this transformation. I led the development of an automation tool that revolutionized the traditional methods of test creation and assessment, aligning perfectly with the modern educational needs.\nMy Role: As a product manager, my role was multifaceted. I was not only the brainchild behind this initiative but also the driving force in its execution. My responsibility encompassed everything from conceptualization to overseeing the development and implementation of the tool.\nThe Challenge: Overcoming Manual Overheads and Generic Assessments The initial challenge was evident. Teachers within our system were burdened with the task of manually creating tests, quizzes, and assessments. This process was time-consuming, labor-intensive, and lacked personalization for individual learners. The need for a system that could alleviate this overhead and cater to the unique learning paths of students was clear.\nThe Solution: A Seamless Integration of Systems To address this, I led the development of an automatic test builder. This innovative tool bridged various systems - Learning Management System (LMS), Content Management System (CMS), Quiz modules, and the Learner App. It functioned by selecting questions based on a pre-defined template and the individual skill levels of students, creating a tailor-made assessment.\nProduct Features: Personalization and Flexibility Personalized Homework: After each class, the tool generated homework tailored to the topics covered, directly assigned to the students who attended the class.\nMonthly Tests: It created monthly tests for batches, focusing on the topics taught during that month.\nAdaptable Assessment Modes: The assessments could be set in either online or offline modes, offering flexibility in delivery and accessibility.\nMetrics and Results: A Leap in Operational Efficiency The most significant achievement of this tool was the drastic reduction in operational manhours - a cut down of over 3000 hours per week. This efficiency not only freed up valuable time for educators but also ensured that assessments were more aligned with the individual learning trajectories of students, resulting in ~20 percent point improvement in attempt rates and ~7 percent point improvement in student outcome.\n","permalink":"http://localhost:52022/blog/automated-assessments/","summary":"\u003cp\u003e\u003cem\u003eIn the ever-evolving landscape of education, the integration of technology has become paramount. My journey as a product manager at Byju\u0026rsquo;s, a leading ed-tech company, reflects this transformation. I led the development of an automation tool that revolutionized the traditional methods of test creation and assessment, aligning perfectly with the modern educational needs.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ree\" loading=\"lazy\" src=\"https://static.wixstatic.com/media/f3d706_48f46023467c490fa5aa7e25517a838d~mv2.png/v1/fill/w_1480,h_846,al_c,q_90,usm_0.66_1.00_0.01,enc_avif,quality_auto/f3d706_48f46023467c490fa5aa7e25517a838d~mv2.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"my-role\"\u003eMy Role:\u003c/h2\u003e\n\u003cp\u003eAs a product manager, my role was multifaceted. I was not only the brainchild behind this initiative but also the driving force in its execution. My responsibility encompassed everything from conceptualization to overseeing the development and implementation of the tool.\u003c/p\u003e","title":"Revolutionizing Education with Automated Assessment - A Journey in Innovation in Byjus"},{"content":"Let\u0026rsquo;s be honest. As a product manager, your to-do list is already miles long, and finding time to read books might seem like a luxury you can\u0026rsquo;t afford. I get it. I\u0026rsquo;ve been there too. But before you dismiss the idea completely, let me share four books that have managed to change my perspective on product management and books in general. These aren\u0026rsquo;t your typical run-of-the-mill, generic recommendations at the CEO or founder level like \u0026ldquo;Lean Product\u0026rdquo;, \u0026ldquo;Hooked\u0026rdquo;, \u0026ldquo;Launch\u0026rdquo; or \u0026ldquo;Ship it\u0026rdquo;, No. these four books provide the kind of food for thought and actionable ideas that you can start thinking about, experimenting with, and implementing right away, regardless of the stage you are currently in your product management career.\n\u0026ldquo;Product Management in Practice: A Real-World Guide to the Key Connective Role of the 21st Century\u0026rdquo; by Matt LeMay: Matt LeMay, in his book \u0026ldquo;Product Management in Practice,\u0026rdquo; offers a refreshing take on product management. LeMay dives deep into the practical aspects of product management, providing real-world examples and strategies to implement.\nIt can seem impossible to learn the nuances of day-to-day product work without doing it yourself. Matt LeMay weaves together case studies from experienced product mangers to help teach and reinforce key dimmensions of the role.\nWhat makes this unique is how it goes beyond the jargons and zeroes in on the practical challenges of product management, with super actionable tips. I was smiling and nodding as I read chapter after chapter.\n\u0026ldquo;Product Management in Practice\u0026rdquo; serves as an invaluable resource for tackling the unpredictable nature of daily organizational tasks. Prior to reading this book, I was unaware of the extent of my own knowledge gaps, the pitfalls I was unknowingly succumbing to, and the vast amount of additional knowledge I needed to acquire.\nThe book is replete with practical anecdotes drawn from real-life experiences. I found myself relating to certain situations where I struggled to navigate effectively. By articulating these experiences and shedding light on the common traps associated with them, the book equiped me with the tools to handle similar challenges more adeptly in the future.\nKey Areas Touched upon:\nRedefined CORE connective skills of Product Management (Communication, Organization, Research, Execution) Importance of Curiosity and Why Best Practices Suck Effective communication and stakeholder management Understanding Vision, Mission, Objectives, Roadmaps, Prioritization, Agile - all ambiguous terms that flow across a Product Organization, how they can have different meanings in different organizations and how a PM can navigate them. Understanding the power of user research and data Management vs Leadership in Product \u0026ldquo;Product Roadmaps Relaunched\u0026rdquo; by C. Todd Lombardo, Bruce McCarthy, and others It’s about time someone brought product roadmapping out of the dark ages of waterfall development and made it into the strategic communications tool it should be. McCarthy and team have cracked the code.\nWe\u0026rsquo;ve all seen those dreaded, outdated roadmaps that promise features set in stone. Lombardo and McCarthy\u0026rsquo;s book, \u0026ldquo;Product Roadmaps Relaunched,\u0026rdquo; challenges that approach and advocates for a more flexible, outcome-focused roadmap for the new age agile environment. They emphasize the need for product managers to engage stakeholders, prioritize ruthlessly, and communicate the ever-evolving nature of product development.\nKey Areas Touched upon:\nA roadmap is a living document, not a contract. Embrace change and adaptability. The power of involving stakeholders throughout the roadmap process to ensure alignment. How to create outcome-based roadmaps that focus on customer value and business goals. \u0026ldquo;Continuous Discovery Habits\u0026rdquo; by Teresa Torres In \u0026ldquo;Continuous Discovery Habits,\u0026rdquo; Teresa Torres presents a compelling argument for making customer research and discovery an ongoing, integral part of the product development process. Torres provides a framework to conduct effective interviews, analyze data, and iterate quickly based on user feedback.\nKey Areas Touched upon:\n\u0026ldquo;The only source of truth about your customers\u0026rsquo; needs is your customers themselves.\u0026rdquo; How to structure customer interviews to gain deep insights and understand the \u0026ldquo;why\u0026rdquo; behind their actions. The importance of running frequent experiments to validate assumptions and uncover new opportunities. \u0026ldquo;User Story Mapping\u0026rdquo; by Jeff Patton: Jeff Patton\u0026rsquo;s \u0026ldquo;User Story Mapping\u0026rdquo; is a must-read for product managers looking to improve their collaboration with stakeholders as well as development teams. He introduces the concept of story mapping as a powerful technique to align stakeholders, prioritize features, and create a shared understanding of the product\u0026rsquo;s journey.\nKey Areas Touched upon:\nA story map helps you tell the story of your product\u0026rsquo;s future by focusing on user activities and the desired outcomes. How to break down user stories into actionable steps that align with business objectives. The significance of iterative development and continuous refinement of the user story map. These four books have been game-changers for me in my product management journey. The ideas they present are practical, and actionable, and have proven to be incredibly effective. I encourage all product managers, regardless of their experience level, to give them a try. Experiment with the strategies outlined, adapt them to your specific context and see what works for you.\nSo, dust off those bookshelves, charge up your e-reader and let the learning begin. Happy reading, fellow product managers! Cheers to becoming better product managers, one page at a time!\n","permalink":"http://localhost:52022/art-of-pm/4-books-every-product-manager-should-read/","summary":"\u003cp\u003eLet\u0026rsquo;s be honest. As a product manager, your to-do list is already miles long, and finding time to read books might seem like a luxury you can\u0026rsquo;t afford. I get it. I\u0026rsquo;ve been there too. But before you dismiss the idea completely, let me share four books that have managed to change my perspective on product management and books in general. These aren\u0026rsquo;t your typical run-of-the-mill, generic recommendations at the CEO or founder level like \u0026ldquo;Lean Product\u0026rdquo;, \u0026ldquo;Hooked\u0026rdquo;, \u0026ldquo;Launch\u0026rdquo; or \u0026ldquo;Ship it\u0026rdquo;, No. these four books provide the kind of food for thought and actionable ideas that you can start thinking about, experimenting with, and implementing right away, regardless of the stage you are currently in your product management career.\u003c/p\u003e","title":"4 Books Every Product Manager Should Read (Even If You Hate Reading Books)"},{"content":"A Streamlit app for demand forecasting with an intuitive, interactive interface. 📱 App Link Key Features\nUses SARIMAX and Holt-Winters methods Allows live parameter configuration Provides real-time visualizations This app allows users to upload time series data and forecast using ARIMA and Holt-Winters (Triple Exponential Smoothing) methods. Navigate to the Forecast page to get started.\nHolt-Winters Triple Exponential Smoothing Holt-Winters Triple Exponential Smoothing, also known as Holt-Winters Method, is used for forecasting time series data that exhibits both trend and seasonality. The model requires you to set values for trend, seasonal, and seasonal_periods. Here’s a guide on how to determine these values:\nUnderstanding the Parameters: Trend (trend): This parameter indicates whether a trend component is included in the model. It can be: 'add' (additive): When the trend is expected to increase or decrease by a constant amount. 'mul' (multiplicative): When the trend increases or decreases by a percentage or ratio. None: When there is no trend. Seasonal (seasonal): This parameter indicates whether a seasonal component is included in the model. It can be: 'add' (additive): When seasonal variations are roughly constant over time. 'mul' (multiplicative): When seasonal variations change proportionally to the level of the time series. None: When there is no seasonality. Seasonal Periods (seasonal_periods): This parameter indicates the length of the seasonality cycle (e.g., 12 for monthly data with annual seasonality). Selecting the Parameters: a. Identify Trend and Seasonality: Visual Inspection: Plot your time series data. Look for patterns in the data: A consistent upward or downward slope suggests a trend. Repeating patterns at regular intervals suggest seasonality. Statistical Tests: Use autocorrelation plots (ACF) and partial autocorrelation plots (PACF) to identify seasonality. Significant spikes at lags corresponding to the seasonal period can indicate seasonality. b. Determine Seasonal Periods: Domain Knowledge: Use domain knowledge to identify the seasonal cycle. For instance, monthly data often has an annual seasonality period of 12 months. Autocorrelation: Significant autocorrelations at specific lags can help identify the seasonal period. c. Choosing Trend and Seasonal Types: Additive vs. Multiplicative: Use additive ('add') if the seasonal variations are roughly constant over time (the difference between high and low seasonality periods is constant). Use multiplicative ('mul') if the seasonal variations change proportionally with the level of the series (the percentage change is constant). SARIMAX Model SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors) is an extension of the ARIMA model that supports both seasonality and exogenous variables. It is used for forecasting time series data. The model parameters need to be carefully chosen to fit the data well. Here’s a guide on understanding and selecting these parameters:\nUnderstanding the Parameters: Non-Seasonal Components: p: The number of lag observations included in the model (autoregressive part). d: The number of times that the raw observations are differenced (integrated part). q: The size of the moving average window. Seasonal Components: P: The number of lag observations in the seasonal model (seasonal autoregressive part). D: The number of times that the seasonal differences are applied (seasonal integrated part). Q: The size of the moving average window in the seasonal model. m: The number of time steps for a single seasonal period. Exogenous Variables (X): These are additional variables that can be included in the model to provide more information that might affect the time series. Selecting the Parameters: a. Identify Non-Seasonal Parameters (p, d, q):\nVisual Inspection: Plot your time series data to look for trends and seasonality. ACF and PACF Plots: ACF (Autocorrelation Function): Helps in identifying the number of MA (q) terms. Significant spikes outside the confidence interval suggest the lag values. PACF (Partial Autocorrelation Function): Helps in identifying the number of AR (p) terms. Significant spikes outside the confidence interval suggest the lag values. Differencing: If the time series is non-stationary, apply differencing and observe the ACF and PACF plots again. Differencing helps to stabilize the mean of the time series by removing changes in the level of a time series, thereby eliminating (or reducing) trend and seasonality. b. Identify Seasonal Parameters (P, D, Q, m):\nSeasonal Differencing: If seasonality is present, seasonal differencing may be necessary. Seasonal ACF and PACF: Look at the ACF and PACF plots of the seasonally differenced series to identify seasonal AR (P) and MA (Q) terms. The seasonal period (m) should be known from the data (e.g., m = 12 for monthly data with yearly seasonality). c. Combining the Parameters:\nUse the identified non-seasonal and seasonal parameters to build the SARIMAX model. Exogenous Variables: Include any relevant external variables (X) if they are believed to influence the time series. Technologies Used: SARIMAX, Holt-Winters, Python, Streamlit\n🔗 GitHub ","permalink":"http://localhost:52022/projects/time-series-forecasting-app/","summary":"Interactive forecasting app using SARIMAX and Holt-Winters models with real-time visualizations.","title":"Time Series Forecasting App"},{"content":"An Experiment to disguise Open Source LLM as a Hotel Bookings Agent!! TL;DR Harnessing the power of open-source LLMs like Llama 3.1, I\u0026rsquo;ve tried to create a chatbot that simplifies hotel bookings, using \u0026ldquo;llama-3.1-8b-instant\u0026rdquo; model, Groq for enhanced AI performance and Streamlit for a seamless web interface. The chatbot can answer user queries, remember chat contexts, take and manage bookings.\n🎥 Watch the demo video 🔗 Check out the application on Streamlit Cloud Background In the ever-evolving world of artificial intelligence, the advent of open-source Large Language Models (LLMs) like DCLM and Llama 3.1 has been looked at as a game-changer. But without real-world application, a good technology is like a work of art, valuable but not tangible.\nFor the past few days, I\u0026rsquo;ve been contemplating how we can harness these powerful tools. Coincidentally, while planning a weekend getaway, I found myself booking a hotel room through a WhatsApp conversation with a human receptionist. This got me thinking: can we use an LLM to handle bookings just as effectively? It seemed like a long shot, but I decided to give it a try.\nDiscovering the Tools: Groq and Streamlit During my research, I came across Groq and Streamlit:\nGroq: Imagine a super-efficient engine designed to accelerate machine learning models. Groq\u0026rsquo;s hardware and software solutions enhance the performance of AI applications, making them faster and more reliable. [ https://groq.com/ ] Streamlit: Think of Streamlit as a magic wand that turns your data scripts into shareable web apps. It\u0026rsquo;s a framework that allows you to create interactive websites from simple Python scripts, making data insights accessible and engaging. And streamlit-cloud makes the hosting a breeze. [ https://streamlit.io/ ] Building the Chatbot With these tools in hand and a little help from ChatGPT, I embarked on creating a chatbot to assist users with hotel bookings.\nThe Flow of the Code:\nHotel Information in JSON: The hotel details, including room types, prices, availability, and amenities, are structured in a JSON format. This ensures that the data is easily accessible and manageable. Passing Context to the LLM: This JSON data is passed as context to the LLM model (\u0026ldquo;llama-3.1-8b-instant\u0026rdquo;) via the Groq API, enabling the chatbot to respond with accurate and relevant information. Maintaining Chat Sessions: Consecutive chats in the session are also passed to the Groq API, ensuring that the bot remembers the flow of the conversation and maintains context throughout the interaction. Streamlit as the FrontEnd: used streamlit to provide a forntend to the interaction happening in the chat 👉 Check out the code in GitHub Where can it lead to? Dynamic Data Integration: While I used static information, the system can easily be adapted to pull live data from various sources, keeping the chatbot updated with current prices and availability. WhatsApp, Call? why not: Although I hosted the application on Streamlit, we can scale it, manage webhooks with ngrok , integrate it with platforms like Twilio to manage WhatsApp/SMS/Call interactions, or use text-to-speech AIs like vapi.ai for real-time conversations. Automated Booking Management: The chatbot can update booking information in a database, send confirmation emails to users, and remember past contexts from sessions to handle any follow-up queries seamlessly. Potential Use Cases: The applications of this technology extend far beyond the hospitality industry. Here are a few exciting possibilities:\nHealthcare: Streamlining patient appointments and managing inquiries. Retail: Assisting customers with product searches and order placements. Finance: Providing customer support for banking services and loan applications. Education: Facilitating enrollment processes and answering student queries. The Advantages of LLM based chat bots: The chatbot will respond naturally, so users won\u0026rsquo;t feel like they\u0026rsquo;re talking to a bot or navigating an IVR system. The user won\u0026rsquo;t be prompted to select one from the below options. The LLM can adapt to the conversation and provide personalized responses each time, enhancing the user experience. We don\u0026rsquo;t need to have huge set of data/ sample interactions to train the chatbots. Though they certainly be beneficial, but we can start fast, reducing the time to market. The bot can be provided with basic context and data, and it would perform quite well. This wouldn\u0026rsquo;t have been possible without this open sourced LLMs. Technologies Used: LLM, Groq API, Streamlit\n🔗 GitHub 📝 Article 📱 App Link ","permalink":"http://localhost:52022/projects/llm-driven-hotel-booking-chatbot/","summary":"Real-time conversational booking assistant powered by Groq API and Streamlit.","title":"LLM-Driven Hotel Booking Chatbot"},{"content":"Introduction In the bustling urban landscape, efficient traffic management is crucial for the smooth functioning of cities. Traffic signal control plays a pivotal role in regulating the flow of vehicles at intersections, ensuring safety, and minimizing congestion. To address the complexities of traffic signal coordination, I introduce a web application that empowers traffic engineers and city planners to optimize signal durations based on real-time traffic flow data.\nProblem Statement Traditional traffic signal systems often operate on fixed schedules, neglecting the dynamic nature of traffic patterns. This rigidity can lead to unnecessary delays, increased fuel consumption, and heightened environmental impact. Furthermore, the lack of tools to easily customize signal durations for specific intersections and traffic conditions poses a challenge for traffic management authorities.\nSolution This web application provides a comprehensive solution to the aforementioned challenges, offering a user-friendly interface and advanced functionalities. The system allows users to define and customize traffic signal timings at a granular level, considering the number of directions, specific coordinates for each direction, and real-time traffic flow data. (This is a personal project)\nProduct Features 1. Interactive Map Selection Users can select the location of a traffic signal crossing with the help of an interactive Bing Map interface. 2. Direction Input The application guides users through the process of inputting the number of directions traffic flows across the crossing.\nEach direction is uniquely identified, allowing for precise customization.\n3. Coordinate Mapping For each direction, users can easily input \u0026ldquo;from\u0026rdquo; and \u0026ldquo;to\u0026rdquo; coordinates by selecting points on the map.\nThis feature streamlines the process of defining the geographic scope of each traffic flow.\n4. Cycle Time Configuration Users can set the total cycle time for the traffic signal, offering flexibility in adjusting the overall timing based on specific requirements.\nThe default cycle time is calculated as \u0026rsquo;number of directions\u0026rsquo; times 90 seconds, providing a starting point for customization.\n5. Dynamic Traffic Flow Calculation The system integrates with bing API to fetch real-time traffic flow data between specified coordinates.\nThis data is used to dynamically calculate signal durations for each direction.\n6. Result Display The application presents the calculated signal durations in a clear and concise tabular format.\nUsers can easily interpret and implement the recommended signal timings for optimized traffic management.\nConclusion This Traffic Signal Control web application serves as a foundational step towards transforming traffic management in urban environments. While the current system empowers users to optimize signal durations at a specific intersection, its potential for scalability is a key aspect that can revolutionize citywide traffic flow.\nExtending the System to Configure Citywide Traffic Lights As cities evolve, the need for comprehensive traffic management systems becomes paramount. The simplicity and effectiveness of our web application pave the way for scaling the system to configure all traffic lights within a city. By expanding the tool\u0026rsquo;s capabilities to encompass multiple intersections, city planners and traffic engineers can holistically address traffic challenges on a macro level.\nAutomated Updates for Enhanced Efficiency Furthermore, envisioning a future where this system integrates with real-time traffic data and machine learning algorithms allows for automated updates. By leveraging data-driven insights, the system can continuously learn and adapt, reducing the reliance on manual interventions. This not only optimizes traffic flow but also enhances the overall efficiency of the transportation infrastructure.\nAdvantages of Reduced Human Intervention The automation of traffic signal configurations for an entire city comes with several advantages. It minimizes the risk of human error, ensures consistency in traffic management strategies, and allows for quicker responses to changing traffic patterns. Reduced human intervention translates to improved reliability and responsiveness, ultimately resulting in smoother traffic flow and reduced congestion.\n📱 App Link ","permalink":"http://localhost:52022/projects/automated-traffic-signal-duration-simulator/","summary":"Simulates optimal signal durations using real-time Bing Maps traffic data.","title":"Automated Traffic Signal Duration Simulator"},{"content":"This application simulates the planning of delivery routes for a specified number of locations and delivery agents within Bangalore. The algorithm used is the Vehicle Routing Problem (VRP) solver provided by OR-Tools , an optimization library developed by Google.\nAlgorithm The Vehicle Routing Problem (VRP) is a combinatorial optimization and integer programming problem that seeks to service a number of customers with a fleet of vehicles. It is a generalization of the Traveling Salesman Problem (TSP).\nStep 1: Generate a warehouse location and random delivery locations within Bangalore. Step 2: Use OR-Tools to solve the VRP with the specified number of delivery agents to minimize the total distance traveled. Step 3: Display the routes for each delivery agent on the map and show the total distance traveled. The algorithm ensures that the delivery routes are optimized for minimal travel distance, taking into account the constraints of the VRP.\nKey Features\nUses Google OR-Tools for path optimization Balances workloads and reduces total delivery time Generates interactive route visualizations Technologies Used: OR-Tools, Python, Streamlit\n🔗 GitHub 📱 App Link ","permalink":"http://localhost:52022/projects/delivery-route-optimization/","summary":"Optimizes last-mile delivery routes using OR-Tools and Streamlit.","title":"Delivery Route Optimization"}]