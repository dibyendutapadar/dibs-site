<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Types of problems where ML can be applied | Dibyendu Tapadar</title>
<meta name="keywords" content="AI PM, Tech for PM, Product Manager Portfolio">
<meta name="description" content="Ever wonder how your email app just  knows  what spam is? Or how Netflix uncannily suggests the exact B-movie you were secretly in the mood for? The answer is Machine Learning (ML).
Before ML, software was a bit of a bureaucrat. You had to write explicit, rigid  if-this-then-that  rules for every single possibility. This works fine until the real world, in all its messy glory, shows up. ML flips the script. Instead of feeding a machine rules, you feed it  examples. The machine&rsquo;s job is to look at thousands or millions of examples and figure out the patterns on its own.">
<meta name="author" content="Dibyendu Tapadar">
<link rel="canonical" href="https://dibyendupm.com/tech-for-pm/types-of-problems-where-ml-can-be-applied/">
<meta property="og:title" content="Types of problems where ML can be applied">
<meta property="og:description" content="Ever wonder how your email app just  knows  what spam is? Or how Netflix uncannily suggests the exact B-movie you were secretly in the mood for? The answer is Machine Learning (ML).
Before ML, software was a bit of a bureaucrat. You had to write explicit, rigid  if-this-then-that  rules for every single possibility. This works fine until the real world, in all its messy glory, shows up. ML flips the script. Instead of feeding a machine rules, you feed it  examples. The machine&rsquo;s job is to look at thousands or millions of examples and figure out the patterns on its own.">
<meta property="og:image" content="">
<meta property="og:url" content="https://dibyendupm.com/tech-for-pm/types-of-problems-where-ml-can-be-applied/">
<meta name="twitter:card" content="summary_large_image">
<meta name="google-site-verification" content="Ksm-dFTFzTkmFmz7MeabjBSLn7pUf19yeCjIFhQXPUM" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.778196ed6ea6927001e4ec13975e74bafb253d50a9dced5b03a16af10f443477.css" integrity="sha256-d4GW7W6mknAB5OwTl150uvslPVCp3O1bA6Fq8Q9ENHc=" rel="preload stylesheet" as="style">

<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5HCHJ7G6');</script>





<script async src="https://www.googletagmanager.com/gtag/js?id=G-TJTELRQR4M"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TJTELRQR4M');
</script>
<link rel="icon" href="https://dibyendupm.com/img/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://dibyendupm.com/img/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://dibyendupm.com/img/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://dibyendupm.com/img/apple-touch-icon.png">
<link rel="mask-icon" href="https://dibyendupm.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://dibyendupm.com/tech-for-pm/types-of-problems-where-ml-can-be-applied/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>mermaid.initialize({ startOnLoad: true });</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" crossorigin="anonymous"
  onload="renderMathInElement(document.body, {
    delimiters: [
      { left: '$$', right: '$$', display: true },
      { left: '$', right: '$', display: false },
      { left: '\\(', right: '\\)', display: false },
      { left: '\\[', right: '\\]', display: true }
    ],
    throwOnError: false
  });"></script>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-83JNZXFBJD"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-83JNZXFBJD');
        }
      </script><meta property="og:url" content="https://dibyendupm.com/tech-for-pm/types-of-problems-where-ml-can-be-applied/">
  <meta property="og:site_name" content="Dibyendu Tapadar">
  <meta property="og:title" content="Types of problems where ML can be applied">
  <meta property="og:description" content="Ever wonder how your email app just knows what spam is? Or how Netflix uncannily suggests the exact B-movie you were secretly in the mood for? The answer is Machine Learning (ML).
Before ML, software was a bit of a bureaucrat. You had to write explicit, rigid if-this-then-that rules for every single possibility. This works fine until the real world, in all its messy glory, shows up. ML flips the script. Instead of feeding a machine rules, you feed it examples. The machine’s job is to look at thousands or millions of examples and figure out the patterns on its own.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="tech-for-pm">
    <meta property="article:published_time" content="2025-11-21T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-21T00:00:00+00:00">
    <meta property="article:tag" content="AI">
    <meta property="og:image" content="https://dibyendupm.com/img/ml.gif">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://dibyendupm.com/img/ml.gif">
<meta name="twitter:title" content="Types of problems where ML can be applied">
<meta name="twitter:description" content="Ever wonder how your email app just  knows  what spam is? Or how Netflix uncannily suggests the exact B-movie you were secretly in the mood for? The answer is Machine Learning (ML).
Before ML, software was a bit of a bureaucrat. You had to write explicit, rigid  if-this-then-that  rules for every single possibility. This works fine until the real world, in all its messy glory, shows up. ML flips the script. Instead of feeding a machine rules, you feed it  examples. The machine&rsquo;s job is to look at thousands or millions of examples and figure out the patterns on its own.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Tech for PMs",
      "item": "https://dibyendupm.com/tech-for-pm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Types of problems where ML can be applied",
      "item": "https://dibyendupm.com/tech-for-pm/types-of-problems-where-ml-can-be-applied/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Types of problems where ML can be applied",
  "name": "Types of problems where ML can be applied",
  "description": "Ever wonder how your email app just knows what spam is? Or how Netflix uncannily suggests the exact B-movie you were secretly in the mood for? The answer is Machine Learning (ML).\nBefore ML, software was a bit of a bureaucrat. You had to write explicit, rigid if-this-then-that rules for every single possibility. This works fine until the real world, in all its messy glory, shows up. ML flips the script. Instead of feeding a machine rules, you feed it examples. The machine\u0026rsquo;s job is to look at thousands or millions of examples and figure out the patterns on its own.\n",
  "keywords": [
    "AI PM", "Tech for PM", "Product Manager Portfolio"
  ],
  "articleBody": "Ever wonder how your email app just knows what spam is? Or how Netflix uncannily suggests the exact B-movie you were secretly in the mood for? The answer is Machine Learning (ML).\nBefore ML, software was a bit of a bureaucrat. You had to write explicit, rigid if-this-then-that rules for every single possibility. This works fine until the real world, in all its messy glory, shows up. ML flips the script. Instead of feeding a machine rules, you feed it examples. The machine’s job is to look at thousands or millions of examples and figure out the patterns on its own.\nIn this post, we’ll skip the scary math and build your intuition for the four most common jobs we give to traditional ML:\nClassification: Is this A or B? Regression: How much of this will there be? Clustering: What are the natural groups in this data? Recommendations: Since you liked that, you’ll probably like this. Let’s dive in.\nClassification: The Art of Sorting Your Data Think of a classification model as a sorting hat for your data. Its one job is to look at a new piece of data and assign it to a predefined category or “class.” This is a type of supervised learning, which is a fancy way of saying we have to give the machine an answer key to learn from first.\nExample: Will This User Pay Up?\nA classic business problem: you have a ton of users on a free plan. Which ones are likely to convert to a paid subscription? This is a perfect sorting problem. The only two buckets are “Yes, they’ll convert” and “No, they won’t.”\nTo teach the machine, we’d give it a pile of historical data that looks something like this:\nUser ID Time Spent (hours) Features Used (count) Support Tickets (count) Last Login (days ago) Converted (Target) 101 5.2 3 1 5 Yes 102 1.1 1 0 20 No 103 8.5 5 2 2 Yes 104 2.0 2 0 15 No 105 6.1 4 1 3 Yes How does the algorithm work?\nAny algorithm is a three-step dance:\nPrep the Data: You gather all your historical user data, including that all-important Converted column (the “answer key”). You clean it up and select the features—the input signals—that you think might predict the outcome. This is more art than science. Train the Model: You feed this labeled data to a classification algorithm (its name doesn’t matter for now). The algorithm’s goal is to find the patterns. It might learn that users who spend more time on the app, use more features, and interact less with support are more likely to convert. It’s essentially drawing a line in the sand that best separates the “Yes” crowd from the “No” crowd. Make Predictions: Once the model is trained, it’s ready for the real world. You show it a new free user who has spent 4.5 hours, used 3 features, and last logged in 7 days ago. The model looks at this new data, compares it to the patterns it learned, and spits out a prediction: “Yes” or “No,” often with a confidence score. Regression: Predicting a Number Okay, sorting is cool. But what if you don’t need to put something in a bucket? What if you need to predict a specific, continuous number? That’s where regression comes in. It’s also supervised learning, but instead of predicting a label (Yes/No), it predicts a value (like $55,000).\nExample: How Much Will We Spend?\nImagine you’re trying to forecast next year’s operating expenses. You know that hiring more people costs more money, but by how much exactly? You want a model that takes your hiring plan and predicts the total cost.\nYour training data might look like this:\nMonth Department Level Planned Headcount Previous Quarter Expenses Operating Expenses (Target) Jan Engineering Senior 15 50,000 55,000 Jan Sales Junior 25 30,000 33,000 Feb Engineering Junior 10 45,000 48,000 Feb Marketing Manager 5 20,000 22,000 Mar Sales Senior 20 35,000 38,000 So, what features do we need?\nHeadcount Info: The number of hires, by department and seniority. Senior engineers cost more than junior sales, for example. Departmental Nuance: Engineering costs might behave differently from Marketing costs. Historical Expenses: Last quarter’s expenses are often a solid baseline. Other Cost Drivers: Think office space, IT licenses, whatever else moves the needle. How does it work?\nPretty similar to classification, just with a different goal:\nPrep the Data: Collect historical data connecting your inputs (headcount, department, level, previous expenses) to your output (actual operating expenses). Clean and format it so the algorithm can chew on it. Train the Model: Feed this data to a regression algorithm. Instead of finding a line to separate data points, it tries to find a mathematical equation that best fits the data points. It’s looking for a predictable relationship. It might learn that, on average, hiring five new senior engineers correlates with an approximate $10,000 increase in operating expenses. Make Predictions: Now, you can give the model your future hiring plan: “We’re planning to hire 20 new engineers (split junior/senior) and 8 new junior salespersons next quarter.” The model uses the formula it learned to crunch the numbers and outputs a single dollar amount—your predicted operating expenses. Clustering: Finding the Tribes in Your Data So far, we’ve been giving the machine an answer key (labeled data). But what if you don’t have one? What if you just have a mountain of data and a hunch that there are natural groups hidden inside? This is unsupervised learning, and its most common tool is clustering.\nThe goal is simple: group similar things together. The algorithm figures out what “similar” means on its own.\nExample: Who Are Your Users, Really?\nA software company wants to understand its user base better. Are they all the same, or are there different “tribes” of users who behave differently? This helps them tailor features, marketing, and support.\nYou start with raw, unlabeled behavioral data, focusing on metrics like:\nUser ID Logins Per Week Features Used Per Session Session Duration (min) Actions Per Session Cluster (Assigned) U001 7 8 45 25 Power User U002 1 2 10 5 Inactive User U003 4 5 20 15 Intermittent User U004 6 7 40 22 Power User U005 2 3 15 8 Degrading User U006 0 1 5 2 Inactive User U007 3 4 18 12 Intermittent User What features do we need?\nUsage Frequency: How often do they log in? Feature Adoption: How many different parts of the app do they touch? Session Metrics: How long do they stay? How much do they do in a session? Engagement Depth: Are they just logging in, or are they really digging into core functions? How does a Clustering Algorithm Work?\nPrep the Data: Gather all your user behavior data. Remember, no predefined ‘Cluster’ column here—the algorithm makes those up. We just numericalize our chosen features. Run the Algorithm: You unleash a clustering algorithm (like the popular K-Means) on the data. Imagine it works like this: The algorithm starts by randomly placing ‘k’ cluster centers (centroids) in your data space. Think of them as initial “leaders” for groups. It then iteratively assigns each data point (a user) to the nearest centroid. After assignment, it recalculates each centroid’s position based on the average of all points assigned to it. The “leader” moves to the center of its group. This process repeats: users get reassigned to their closest new leader, and leaders move again. This continues until the centroids stabilize, meaning users no longer shift between clusters. Interpret the Clusters: The machine doesn’t name the groups; it just creates them. It’s up to a human to look at the users in each cluster and give them a meaningful label. You might find: Power Users: High login frequency, extensive feature use, long sessions. Intermittent Users: Moderate usage, sporadic engagement. Degrading Users: Declining usage patterns, fewer features used over time. Inactive Users: Very low or no recent activity. Recommendation Systems: “You Might Also Like…” You already know this one. It’s the engine behind Amazon, Netflix, and Spotify. Recommendation systems predict what a user might like based on their past behavior and the behavior of similar users. Let’s look at two traditional approaches for a streaming service recommending movies.\nApproach 1: Collaborative Filtering (The Wisdom of the Crowds)\nThe core idea: “People who liked what you liked also liked…” It doesn’t need to know anything about the movies themselves, just who watched what. The data is typically structured as a user-item interaction matrix.\nUser ID Movie A (e.g., “Inception”) Movie B (e.g., “The Dark Knight”) Movie C (e.g., “Shrek”) Movie D (e.g., “Parasite”) Alice 1 1 0 1 Bob 1 1 0 0 Charlie 0 0 1 0 David 1 0 0 1 Eve 0 0 1 0 (A ‘1’ indicates the user has watched or liked the movie; ‘0’ indicates they have not.)\nHow a Collaborative Filtering Algorithm Works:\nUser-Item Matrix Creation: First, we build a giant grid where rows are users and columns are movies. Each cell notes if a user watched/liked a movie. Finding Similarities: The algorithm then hunts for similarities, either between users or between movies: User-Based Similarity: It finds users whose viewing habits closely match the target user. If Bob and Alice watched the same collection of action movies, they’re “similar.” Item-Based Similarity: It focuses on finding movies that are frequently enjoyed by the same group of people. If users who liked “Inception” also commonly liked “The Dark Knight,” these two movies are considered similar. Generating Recommendations: User-Based Approach: If Bob watched “Inception” and “The Dark Knight,” and Alice watched those plus “Parasite,” the system might recommend “Parasite” to Bob because Alice is a “taste-twin.” Item-Based Approach: If a user watches “The Dark Knight,” the system looks for other movies commonly watched by people who liked “The Dark Knight.” If “Inception” comes up often, it’s a good bet for the current user. Training: Algorithms like K-Nearest Neighbors (KNN) or matrix factorization (like SVD) are used to crunch these similarities efficiently. Visualizing the Collaborative Filtering Flow:\ngraph TD A[\"User Interaction Data such as Movie Watch History\"] --\u003e B{\"Create User Item Matrix\"}; B --\u003e C[\"Matrix: Users x Movies\"]; C --\u003e D{\"Calculate Similarity (User User or Item Item)\"}; D --\u003e E[\"Find Similar Users or Items\"]; E --\u003e F{\"Generate Recommendations\"}; F --\u003e G[\"Recommended Movies for User\"]; Approach 2: Content-Based Filtering (If You Like Apples…)\nThe core idea: “You liked this thing, so you’ll probably like other things with similar attributes.” This method cares deeply about the content of the items themselves.\nMovie ID Title Genre Director Actors (Top 2) M01 Inception Sci-Fi, Thriller C. Nolan L. DiCaprio, E. Page M02 The Dark Knight Action, Crime C. Nolan C. Bale, H. Ledger M03 Shrek Animation, Comedy A. Adamson M. Myers, E. Murphy M04 Parasite Drama, Thriller B. Joon-ho S. Kang, T. Choi M05 Interstellar Sci-Fi, Drama C. Nolan M. McConaughey, A. Hathaway How a Content-Based Filtering Algorithm Works:\nItem Profiling: Each movie gets a detailed “profile” based on its characteristics—genre, director, actors, even keywords. User Profiling: The system then builds a profile for you, based on the movies you’ve liked. If you loved “Inception” and “Interstellar,” your profile might scream “Sci-Fi,” “Christopher Nolan,” and “Thriller/Drama.” Recommendation Generation: The algorithm compares your profile to all the unseen movie profiles. It then recommends the movies whose attributes most closely align with your established preferences. For instance, a Sci-Fi fan who loves Christopher Nolan would definitely get “Interstellar” suggested. Visualizing the Content-Based Filtering Flow:\ngraph TD A[\"Movie Data (Genre, Director, Actors, etc.)\"] --\u003e B{\"Create Item Profiles\"}; B --\u003e C[\"Item Profiles\"]; C --\u003e D[\"User's Liked Items\"]; D --\u003e E{\"Create User Profile\"}; E --\u003e F[\"User Profile\"]; F --\u003e G[\"Compare User Profile with Unseen Item Profiles\"]; G --\u003e H{\"Generate Recommendations\"}; H --\u003e I[\"Recommended Movies\"]; ",
  "wordCount" : "1964",
  "inLanguage": "en",
  "image":"https://dibyendupm.com/img/ml.gif","datePublished": "2025-11-21T00:00:00Z",
  "dateModified": "2025-11-21T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Dibyendu Tapadar"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://dibyendupm.com/tech-for-pm/types-of-problems-where-ml-can-be-applied/"
  },
  "publisher": {
    "@type": "Person",
    "name": "Dibyendu Tapadar",
    "logo": {
      "@type": "ImageObject",
      "url": "https://dibyendupm.com/img/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">


<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5HCHJ7G6"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://dibyendupm.com/" accesskey="h" title="Dibyendu Tapadar (Alt + H)">Dibyendu Tapadar</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://dibyendupm.com/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://dibyendupm.com/work/" title="Experience">
                    <span><i class="fa-solid fa-briefcase"></i>Experience</span>
                </a>
            </li>
            <li>
                <a href="https://dibyendupm.com/projects/" title="Side Projects">
                    <span>Side Projects</span>
                </a>
            </li>
            <li>
                <a href="https://dibyendupm.com/art-of-pm/" title="Art of PMing">
                    <span>Art of PMing</span>
                </a>
            </li>
            <li>
                <a href="https://dibyendupm.com/tech-for-pm/" title="Tech for PM">
                    <span>Tech for PM</span>
                </a>
            </li>
            <li>
                <a href="https://dibyendupm.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Types of problems where ML can be applied
    </h1>
    
    <div class="post-meta"><span title='2025-11-21 00:00:00 +0000 UTC'>November 21, 2025</span>&nbsp;·&nbsp;<span>10 min</span>&nbsp;·&nbsp;<span>Dibyendu Tapadar</span>

</div>
  </header> 
<figure class="entry-cover">
        <img loading="eager" src="https://dibyendupm.com/img/ml.gif" alt="types-of-problems-where-ml-can-be-applied">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#classification-the-art-of-sorting-your-data" aria-label="Classification: The Art of Sorting Your Data">Classification: The Art of Sorting Your Data</a></li>
                <li>
                    <a href="#regression-predicting-a-number" aria-label="Regression: Predicting a Number">Regression: Predicting a Number</a></li>
                <li>
                    <a href="#clustering-finding-the-tribes-in-your-data" aria-label="Clustering: Finding the Tribes in Your Data">Clustering: Finding the Tribes in Your Data</a></li>
                <li>
                    <a href="#recommendation-systems-you-might-also-like" aria-label="Recommendation Systems: &ldquo;You Might Also Like&hellip;&rdquo;">Recommendation Systems: &ldquo;You Might Also Like&hellip;&rdquo;</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Ever wonder how your email app just  <em>knows</em>  what spam is? Or how Netflix uncannily suggests the exact B-movie you were secretly in the mood for? The answer is Machine Learning (ML).</p>
<p>Before ML, software was a bit of a bureaucrat. You had to write explicit, rigid  <code>if-this-then-that</code>  rules for every single possibility. This works fine until the real world, in all its messy glory, shows up. ML flips the script. Instead of feeding a machine rules, you feed it  <em>examples</em>. The machine&rsquo;s job is to look at thousands or millions of examples and figure out the patterns on its own.</p>
<p>In this post, we&rsquo;ll skip the scary math and build your intuition for the four most common jobs we give to traditional ML:</p>
<ul>
<li><strong>Classification:</strong>  Is this A or B?</li>
<li><strong>Regression:</strong>  How much of this will there be?</li>
<li><strong>Clustering:</strong>  What are the natural groups in this data?</li>
<li><strong>Recommendations:</strong>  Since you liked that, you&rsquo;ll probably like this.</li>
</ul>
<p>Let&rsquo;s dive in.</p>
<h3 id="classification-the-art-of-sorting-your-data">Classification: The Art of Sorting Your Data<a hidden class="anchor" aria-hidden="true" href="#classification-the-art-of-sorting-your-data">#</a></h3>
<p>Think of a classification model as a sorting hat for your data. Its one job is to look at a new piece of data and assign it to a predefined category or &ldquo;class.&rdquo; This is a type of  <strong>supervised learning</strong>, which is a fancy way of saying we have to give the machine an answer key to learn from first.</p>
<p><strong>Example: Will This User Pay Up?</strong></p>
<p>A classic business problem: you have a ton of users on a free plan. Which ones are likely to convert to a paid subscription? This is a perfect sorting problem. The only two buckets are &ldquo;Yes, they&rsquo;ll convert&rdquo; and &ldquo;No, they won&rsquo;t.&rdquo;</p>
<p>To teach the machine, we&rsquo;d give it a pile of historical data that looks something like this:</p>
<table>
  <thead>
      <tr>
          <th>User ID</th>
          <th>Time Spent (hours)</th>
          <th>Features Used (count)</th>
          <th>Support Tickets (count)</th>
          <th>Last Login (days ago)</th>
          <th><strong>Converted (Target)</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>101</td>
          <td>5.2</td>
          <td>3</td>
          <td>1</td>
          <td>5</td>
          <td>Yes</td>
      </tr>
      <tr>
          <td>102</td>
          <td>1.1</td>
          <td>1</td>
          <td>0</td>
          <td>20</td>
          <td>No</td>
      </tr>
      <tr>
          <td>103</td>
          <td>8.5</td>
          <td>5</td>
          <td>2</td>
          <td>2</td>
          <td>Yes</td>
      </tr>
      <tr>
          <td>104</td>
          <td>2.0</td>
          <td>2</td>
          <td>0</td>
          <td>15</td>
          <td>No</td>
      </tr>
      <tr>
          <td>105</td>
          <td>6.1</td>
          <td>4</td>
          <td>1</td>
          <td>3</td>
          <td>Yes</td>
      </tr>
  </tbody>
</table>
<p><strong>How does the algorithm work?</strong></p>
<p>Any algorithm is a three-step dance:</p>
<ol>
<li><strong>Prep the Data:</strong>  You gather all your historical user data, including that all-important  <code>Converted</code>  column (the &ldquo;answer key&rdquo;). You clean it up and select the features—the input signals—that you think might predict the outcome. This is more art than science.</li>
<li><strong>Train the Model:</strong>  You feed this labeled data to a classification algorithm (its name doesn&rsquo;t matter for now). The algorithm&rsquo;s goal is to find the patterns. It might learn that users who spend more time on the app, use more features, and interact less with support are more likely to convert. It&rsquo;s essentially drawing a line in the sand that best separates the &ldquo;Yes&rdquo; crowd from the &ldquo;No&rdquo; crowd.</li>
<li><strong>Make Predictions:</strong>  Once the model is trained, it&rsquo;s ready for the real world. You show it a  <em>new</em>  free user who has spent 4.5 hours, used 3 features, and last logged in 7 days ago. The model looks at this new data, compares it to the patterns it learned, and spits out a prediction: &ldquo;Yes&rdquo; or &ldquo;No,&rdquo; often with a confidence score.</li>
</ol>
<h3 id="regression-predicting-a-number">Regression: Predicting a Number<a hidden class="anchor" aria-hidden="true" href="#regression-predicting-a-number">#</a></h3>
<p>Okay, sorting is cool. But what if you don&rsquo;t need to put something in a bucket? What if you need to predict a specific, continuous  <em>number</em>? That&rsquo;s where regression comes in. It&rsquo;s also supervised learning, but instead of predicting a label (<code>Yes</code>/<code>No</code>), it predicts a value (like  <code>$55,000</code>).</p>
<p><strong>Example: How Much Will We Spend?</strong></p>
<p>Imagine you&rsquo;re trying to forecast next year&rsquo;s operating expenses. You know that hiring more people costs more money, but by how much exactly? You want a model that takes your hiring plan and predicts the total cost.</p>
<p>Your training data might look like this:</p>
<table>
  <thead>
      <tr>
          <th>Month</th>
          <th>Department</th>
          <th>Level</th>
          <th>Planned Headcount</th>
          <th>Previous Quarter Expenses</th>
          <th><strong>Operating Expenses (Target)</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Jan</td>
          <td>Engineering</td>
          <td>Senior</td>
          <td>15</td>
          <td>50,000</td>
          <td>55,000</td>
      </tr>
      <tr>
          <td>Jan</td>
          <td>Sales</td>
          <td>Junior</td>
          <td>25</td>
          <td>30,000</td>
          <td>33,000</td>
      </tr>
      <tr>
          <td>Feb</td>
          <td>Engineering</td>
          <td>Junior</td>
          <td>10</td>
          <td>45,000</td>
          <td>48,000</td>
      </tr>
      <tr>
          <td>Feb</td>
          <td>Marketing</td>
          <td>Manager</td>
          <td>5</td>
          <td>20,000</td>
          <td>22,000</td>
      </tr>
      <tr>
          <td>Mar</td>
          <td>Sales</td>
          <td>Senior</td>
          <td>20</td>
          <td>35,000</td>
          <td>38,000</td>
      </tr>
  </tbody>
</table>
<p><strong>So, what features do we need?</strong></p>
<ul>
<li><strong>Headcount Info:</strong>  The number of hires, by department and seniority. Senior engineers cost more than junior sales, for example.</li>
<li><strong>Departmental Nuance:</strong>  Engineering costs might behave differently from Marketing costs.</li>
<li><strong>Historical Expenses:</strong>  Last quarter&rsquo;s expenses are often a solid baseline.</li>
<li><strong>Other Cost Drivers:</strong>  Think office space, IT licenses, whatever else moves the needle.</li>
</ul>
<p><strong>How does it work?</strong></p>
<p>Pretty similar to classification, just with a different goal:</p>
<ol>
<li><strong>Prep the Data:</strong>  Collect historical data connecting your inputs (headcount, department, level, previous expenses) to your output (actual operating expenses). Clean and format it so the algorithm can chew on it.</li>
<li><strong>Train the Model:</strong>  Feed this data to a regression algorithm. Instead of finding a line to  <em>separate</em>  data points, it tries to find a mathematical equation that  <em>best fits</em>  the data points. It&rsquo;s looking for a predictable relationship. It might learn that, on average, hiring five new senior engineers correlates with an approximate $10,000 increase in operating expenses.</li>
<li><strong>Make Predictions:</strong>  Now, you can give the model your future hiring plan: &ldquo;We&rsquo;re planning to hire 20 new engineers (split junior/senior) and 8 new junior salespersons next quarter.&rdquo; The model uses the formula it learned to crunch the numbers and outputs a single dollar amount—your predicted operating expenses.</li>
</ol>
<h3 id="clustering-finding-the-tribes-in-your-data">Clustering: Finding the Tribes in Your Data<a hidden class="anchor" aria-hidden="true" href="#clustering-finding-the-tribes-in-your-data">#</a></h3>
<p>So far, we&rsquo;ve been giving the machine an answer key (labeled data). But what if you don&rsquo;t have one? What if you just have a mountain of data and a hunch that there are natural groups hidden inside? This is  <strong>unsupervised learning</strong>, and its most common tool is clustering.</p>
<p>The goal is simple: group similar things together. The algorithm figures out what &ldquo;similar&rdquo; means on its own.</p>
<p><strong>Example: Who Are Your Users,  <em>Really</em>?</strong></p>
<p>A software company wants to understand its user base better. Are they all the same, or are there different &ldquo;tribes&rdquo; of users who behave differently? This helps them tailor features, marketing, and support.</p>
<p>You start with raw, unlabeled behavioral data, focusing on metrics like:</p>
<table>
  <thead>
      <tr>
          <th>User ID</th>
          <th>Logins Per Week</th>
          <th>Features Used Per Session</th>
          <th>Session Duration (min)</th>
          <th>Actions Per Session</th>
          <th><strong>Cluster (Assigned)</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>U001</td>
          <td>7</td>
          <td>8</td>
          <td>45</td>
          <td>25</td>
          <td>Power User</td>
      </tr>
      <tr>
          <td>U002</td>
          <td>1</td>
          <td>2</td>
          <td>10</td>
          <td>5</td>
          <td>Inactive User</td>
      </tr>
      <tr>
          <td>U003</td>
          <td>4</td>
          <td>5</td>
          <td>20</td>
          <td>15</td>
          <td>Intermittent User</td>
      </tr>
      <tr>
          <td>U004</td>
          <td>6</td>
          <td>7</td>
          <td>40</td>
          <td>22</td>
          <td>Power User</td>
      </tr>
      <tr>
          <td>U005</td>
          <td>2</td>
          <td>3</td>
          <td>15</td>
          <td>8</td>
          <td>Degrading User</td>
      </tr>
      <tr>
          <td>U006</td>
          <td>0</td>
          <td>1</td>
          <td>5</td>
          <td>2</td>
          <td>Inactive User</td>
      </tr>
      <tr>
          <td>U007</td>
          <td>3</td>
          <td>4</td>
          <td>18</td>
          <td>12</td>
          <td>Intermittent User</td>
      </tr>
  </tbody>
</table>
<p><strong>What features do we need?</strong></p>
<ul>
<li><strong>Usage Frequency:</strong>  How often do they log in?</li>
<li><strong>Feature Adoption:</strong>  How many different parts of the app do they touch?</li>
<li><strong>Session Metrics:</strong>  How long do they stay? How much do they  <em>do</em>  in a session?</li>
<li><strong>Engagement Depth:</strong>  Are they just logging in, or are they really digging into core functions?</li>
</ul>
<p><strong>How does a Clustering Algorithm Work?</strong></p>
<ol>
<li><strong>Prep the Data:</strong>  Gather all your user behavior data. Remember, no predefined &lsquo;Cluster&rsquo; column here—the algorithm makes those up. We just numericalize our chosen features.</li>
<li><strong>Run the Algorithm:</strong>  You unleash a clustering algorithm (like the popular K-Means) on the data. Imagine it works like this:
<ul>
<li>The algorithm starts by randomly placing &lsquo;k&rsquo; cluster centers (centroids) in your data space. Think of them as initial &ldquo;leaders&rdquo; for groups.</li>
<li>It then iteratively assigns each data point (a user) to the nearest centroid.</li>
<li>After assignment, it recalculates each centroid&rsquo;s position based on the average of all points assigned to it. The &ldquo;leader&rdquo; moves to the center of its group.</li>
<li>This process repeats: users get reassigned to their closest new leader, and leaders move again. This continues until the centroids stabilize, meaning users no longer shift between clusters.</li>
</ul>
</li>
<li><strong>Interpret the Clusters:</strong>  The machine doesn&rsquo;t name the groups; it just creates them. It&rsquo;s up to a human to look at the users in each cluster and give them a meaningful label. You might find:
<ul>
<li><strong>Power Users:</strong>  High login frequency, extensive feature use, long sessions.</li>
<li><strong>Intermittent Users:</strong>  Moderate usage, sporadic engagement.</li>
<li><strong>Degrading Users:</strong>  Declining usage patterns, fewer features used over time.</li>
<li><strong>Inactive Users:</strong>  Very low or no recent activity.</li>
</ul>
</li>
</ol>
<h3 id="recommendation-systems-you-might-also-like">Recommendation Systems: &ldquo;You Might Also Like&hellip;&rdquo;<a hidden class="anchor" aria-hidden="true" href="#recommendation-systems-you-might-also-like">#</a></h3>
<p>You already know this one. It&rsquo;s the engine behind Amazon, Netflix, and Spotify. Recommendation systems predict what a user might like based on their past behavior and the behavior of similar users. Let&rsquo;s look at two traditional approaches for a streaming service recommending movies.</p>
<p><strong>Approach 1: Collaborative Filtering (The Wisdom of the Crowds)</strong></p>
<p>The core idea: &ldquo;People who liked what you liked also liked&hellip;&rdquo; It doesn&rsquo;t need to know anything about the movies themselves, just who watched what. The data is typically structured as a user-item interaction matrix.</p>
<table>
  <thead>
      <tr>
          <th>User ID</th>
          <th>Movie A (e.g., &ldquo;Inception&rdquo;)</th>
          <th>Movie B (e.g., &ldquo;The Dark Knight&rdquo;)</th>
          <th>Movie C (e.g., &ldquo;Shrek&rdquo;)</th>
          <th>Movie D (e.g., &ldquo;Parasite&rdquo;)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Alice</td>
          <td>1</td>
          <td>1</td>
          <td>0</td>
          <td>1</td>
      </tr>
      <tr>
          <td>Bob</td>
          <td>1</td>
          <td>1</td>
          <td>0</td>
          <td>0</td>
      </tr>
      <tr>
          <td>Charlie</td>
          <td>0</td>
          <td>0</td>
          <td>1</td>
          <td>0</td>
      </tr>
      <tr>
          <td>David</td>
          <td>1</td>
          <td>0</td>
          <td>0</td>
          <td>1</td>
      </tr>
      <tr>
          <td>Eve</td>
          <td>0</td>
          <td>0</td>
          <td>1</td>
          <td>0</td>
      </tr>
  </tbody>
</table>
<p><em>(A &lsquo;1&rsquo; indicates the user has watched or liked the movie; &lsquo;0&rsquo; indicates they have not.)</em></p>
<p><strong>How a Collaborative Filtering Algorithm Works:</strong></p>
<ol>
<li><strong>User-Item Matrix Creation:</strong>  First, we build a giant grid where rows are users and columns are movies. Each cell notes if a user watched/liked a movie.</li>
<li><strong>Finding Similarities:</strong>  The algorithm then hunts for similarities, either between users or between movies:
<ul>
<li><strong>User-Based Similarity:</strong>  It finds users whose viewing habits closely match the target user. If Bob and Alice watched the same collection of action movies, they&rsquo;re &ldquo;similar.&rdquo;</li>
<li><strong>Item-Based Similarity:</strong>  It focuses on finding movies that are frequently enjoyed by the  <em>same group</em>  of people. If users who liked &ldquo;Inception&rdquo; also commonly liked &ldquo;The Dark Knight,&rdquo; these two movies are considered similar.</li>
</ul>
</li>
<li><strong>Generating Recommendations:</strong>
<ul>
<li><strong>User-Based Approach:</strong>  If Bob watched &ldquo;Inception&rdquo; and &ldquo;The Dark Knight,&rdquo; and Alice watched those  <em>plus</em>  &ldquo;Parasite,&rdquo; the system might recommend &ldquo;Parasite&rdquo; to Bob because Alice is a &ldquo;taste-twin.&rdquo;</li>
<li><strong>Item-Based Approach:</strong>  If a user watches &ldquo;The Dark Knight,&rdquo; the system looks for other movies commonly watched by people who liked &ldquo;The Dark Knight.&rdquo; If &ldquo;Inception&rdquo; comes up often, it&rsquo;s a good bet for the current user.</li>
</ul>
</li>
<li><strong>Training:</strong>  Algorithms like K-Nearest Neighbors (KNN) or matrix factorization (like SVD) are used to crunch these similarities efficiently.</li>
</ol>
<p><strong>Visualizing the Collaborative Filtering Flow:</strong></p>
<div class="mermaid">
graph TD
    A["User Interaction Data such as Movie Watch History"] --> B{"Create User Item Matrix"};
    B --> C["Matrix: Users x Movies"];
    C --> D{"Calculate Similarity (User User or Item Item)"};
    D --> E["Find Similar Users or Items"];
    E --> F{"Generate Recommendations"};
    F --> G["Recommended Movies for User"];
</div><p><strong>Approach 2: Content-Based Filtering (If You Like Apples&hellip;)</strong></p>
<p>The core idea: &ldquo;You liked this thing, so you&rsquo;ll probably like other things with similar attributes.&rdquo; This method cares deeply about the  <em>content</em>  of the items themselves.</p>
<table>
  <thead>
      <tr>
          <th>Movie ID</th>
          <th>Title</th>
          <th>Genre</th>
          <th>Director</th>
          <th>Actors (Top 2)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>M01</td>
          <td>Inception</td>
          <td>Sci-Fi, Thriller</td>
          <td>C. Nolan</td>
          <td>L. DiCaprio, E. Page</td>
      </tr>
      <tr>
          <td>M02</td>
          <td>The Dark Knight</td>
          <td>Action, Crime</td>
          <td>C. Nolan</td>
          <td>C. Bale, H. Ledger</td>
      </tr>
      <tr>
          <td>M03</td>
          <td>Shrek</td>
          <td>Animation, Comedy</td>
          <td>A. Adamson</td>
          <td>M. Myers, E. Murphy</td>
      </tr>
      <tr>
          <td>M04</td>
          <td>Parasite</td>
          <td>Drama, Thriller</td>
          <td>B. Joon-ho</td>
          <td>S. Kang, T. Choi</td>
      </tr>
      <tr>
          <td>M05</td>
          <td>Interstellar</td>
          <td>Sci-Fi, Drama</td>
          <td>C. Nolan</td>
          <td>M. McConaughey, A. Hathaway</td>
      </tr>
  </tbody>
</table>
<p><strong>How a Content-Based Filtering Algorithm Works:</strong></p>
<ol>
<li><strong>Item Profiling:</strong>  Each movie gets a detailed &ldquo;profile&rdquo; based on its characteristics—genre, director, actors, even keywords.</li>
<li><strong>User Profiling:</strong>  The system then builds a profile for  <em>you</em>, based on the movies you&rsquo;ve liked. If you loved &ldquo;Inception&rdquo; and &ldquo;Interstellar,&rdquo; your profile might scream &ldquo;Sci-Fi,&rdquo; &ldquo;Christopher Nolan,&rdquo; and &ldquo;Thriller/Drama.&rdquo;</li>
<li><strong>Recommendation Generation:</strong>  The algorithm compares your profile to all the unseen movie profiles. It then recommends the movies whose attributes most closely align with your established preferences. For instance, a Sci-Fi fan who loves Christopher Nolan would definitely get &ldquo;Interstellar&rdquo; suggested.</li>
</ol>
<p><strong>Visualizing the Content-Based Filtering Flow:</strong></p>
<div class="mermaid">
graph TD
    A["Movie Data (Genre, Director, Actors, etc.)"] --> B{"Create Item Profiles"};
    B --> C["Item Profiles"];
    C --> D["User's Liked Items"];
    D --> E{"Create User Profile"};
    E --> F["User Profile"];
    F --> G["Compare User Profile with Unseen Item Profiles"];
    G --> H{"Generate Recommendations"};
    H --> I["Recommended Movies"];
</div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://dibyendupm.com/tags/ai/">AI</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://dibyendupm.com/">Dibyendu Tapadar</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> |
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a> |
        <a href="https://obsidian.md/" rel="noopener" target="_blank">Obsidian</a> |
        <a href="https://obsidian.md/" rel="noopener" target="_blank">Netlify</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

<script>
document.querySelectorAll('p, li, h1, h2, h3, h4, h5, h6').forEach(node => {
  node.innerHTML = node.innerHTML.replace(/==([^=]+)==/g, '<mark>$1</mark>');
});
</script>
</body>

</html>
